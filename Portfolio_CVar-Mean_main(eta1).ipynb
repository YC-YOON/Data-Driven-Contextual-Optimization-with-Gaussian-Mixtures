{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a183d196053ba7c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T22:54:29.243180Z",
     "start_time": "2025-07-24T22:54:29.239663Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.core.missing import validate_limit_direction\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "from sklearn.neighbors import KernelDensity\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import multivariate_normal\n",
    "from scipy.stats import truncnorm\n",
    "from tqdm import tqdm\n",
    "import cvxpy as cp\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "from joblib import Parallel, delayed\n",
    "import torch\n",
    "import normflows as nf\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "np.set_printoptions(linewidth=200)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "32c107e960973565",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T22:54:29.269985Z",
     "start_time": "2025-07-24T22:54:29.265922Z"
    }
   },
   "outputs": [],
   "source": [
    "# ===================== Deterministic Setup =====================\n",
    "def set_all_seeds(seed: int = 42):\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":16:8\"  \n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    try:\n",
    "        torch.use_deterministic_algorithms(True)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"]=\"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"]=\"1\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"]=\"1\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"]=\"1\"\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":16:8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d00eb2d5f1b4a408",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T22:54:29.300981Z",
     "start_time": "2025-07-24T22:54:29.289514Z"
    }
   },
   "outputs": [],
   "source": [
    "# -- Implementation 3 ---\n",
    "######################### 2-Wass GMM DRO function ###################################\n",
    "def Portfolio_2_Wass(xi, eps):\n",
    "    N, d = xi.shape\n",
    "    lda = cp.Variable(nonneg=True)\n",
    "    s = cp.Variable(N)\n",
    "    x = cp.Variable(d, nonneg=True)\n",
    "\n",
    "    constraints = []\n",
    "    for i in range(N):\n",
    "        constraints.append(\n",
    "            s[i] >= cp.quad_over_lin(x, 4 * lda) - xi[i] @ x\n",
    "        )\n",
    "    constraints.append(cp.sum(x) == 1)\n",
    "\n",
    "    obj = cp.Minimize(lda * (eps ** 2) + (1 / N) * cp.sum(s))\n",
    "    prob = cp.Problem(obj, constraints)\n",
    "    prob.solve(solver=cp.MOSEK, verbose=False)\n",
    "    return x.value\n",
    "\n",
    "def Portfolio_2_Wass_MCVaR(xi, eps, tau, eta):\n",
    "    N, d = xi.shape\n",
    "    lda = cp.Variable(nonneg=True)\n",
    "    s = cp.Variable(N)\n",
    "    x = cp.Variable(d, nonneg=True)\n",
    "    beta = cp.Variable()\n",
    "\n",
    "    const = []\n",
    "    for i in range(N):\n",
    "        xi_norm2 = float(np.sum(xi[i]**2))\n",
    "        const.append(cp.norm2(cp.hstack([2 * lda * xi[i] - ((1 / tau) + eta) * x, lda * xi_norm2 + s[i] + (beta/tau) - beta- lda]))\n",
    "                     <= lda * xi_norm2 + s[i] + (beta/tau) - beta + lda)\n",
    "        const.append(cp.norm2(cp.hstack([2 * lda * xi[i] - eta * x, lda * xi_norm2 + s[i] -  beta - lda]))\n",
    "                     <= lda * xi_norm2 + s[i] - beta + lda)\n",
    "        const.append(lda * xi_norm2 + s[i] >= -(beta/tau) + beta)\n",
    "        const.append(lda * xi_norm2 + s[i] >= beta)\n",
    "    const.append(cp.sum(x) == 1)\n",
    "\n",
    "    obj = cp.Minimize(lda * (eps**2) + (1 / N) * cp.sum(s))\n",
    "    prob = cp.Problem(obj, const)\n",
    "    prob.solve(solver=cp.MOSEK)\n",
    "    return x.value\n",
    "\n",
    "def transforming_conditional(s, num_components, mu_k, sig_k, p_k, dim_s):\n",
    "    mu_cond, cov_cond, weights = [], [], []\n",
    "    for k in range(num_components):\n",
    "        mu = mu_k[k]\n",
    "        sigma = sig_k[k]\n",
    "        mu_s = mu[:dim_s]\n",
    "        mu_xi = mu[dim_s:]\n",
    "        sigma_ss = sigma[:dim_s, :dim_s]\n",
    "        sigma_sx = sigma[:dim_s, dim_s:]\n",
    "        sigma_xs = sigma[dim_s:, :dim_s]\n",
    "        sigma_xx = sigma[dim_s:, dim_s:]\n",
    "        sigma_ss_inv = np.linalg.inv(sigma_ss)\n",
    "        cond_mu = mu_xi + sigma_xs @ sigma_ss_inv @ (s - mu_s)\n",
    "        cond_cov = sigma_xx - sigma_xs @ sigma_ss_inv @ sigma_sx\n",
    "        weight = p_k[k] * multivariate_normal.pdf(s, mean=mu_s, cov=sigma_ss)\n",
    "        mu_cond.append(cond_mu)\n",
    "        cov_cond.append(cond_cov)\n",
    "        weights.append(weight)\n",
    "    weights = np.array(weights)\n",
    "    if np.any(np.isnan(weights)) or weights.sum() <= 1e-12:\n",
    "        weights = np.ones_like(weights) / len(weights)\n",
    "    else:\n",
    "        weights /= weights.sum()\n",
    "    return np.array(mu_cond), np.array(cov_cond), weights\n",
    "\n",
    "def MC_sampling(K, N, mu_list, cov_list, p_list):\n",
    "    d = mu_list.shape[1]\n",
    "    samples = np.zeros((N, d))\n",
    "    for i in range(N):\n",
    "        k = np.random.choice(K, p=p_list)\n",
    "        samples[i] = np.random.multivariate_normal(mu_list[k], cov_list[k])\n",
    "    return samples\n",
    "\n",
    "def oos_loss_portfolio(x, xi, tau, eta):\n",
    "    x = x.reshape(-1)\n",
    "    xi = xi.reshape(-1)\n",
    "    beta = cp.Variable()\n",
    "    yTx = xi @ x\n",
    "    term1 = -eta * yTx + beta\n",
    "    term2 = -(eta + 1 / tau) * yTx + (1 - 1 / tau) * beta\n",
    "    loss_expr = cp.maximum(term1, term2)\n",
    "    prob = cp.Problem(cp.Minimize(loss_expr))\n",
    "    prob.solve(solver=cp.MOSEK, verbose=False)\n",
    "    return prob.value\n",
    "\n",
    "def oos_loss_valid(x, xi, tau, eta):\n",
    "    x = x.reshape(-1)\n",
    "    xi = xi.reshape(-1)\n",
    "    beta = cp.Variable()\n",
    "    yTx = xi @ x\n",
    "    term1 = -eta * yTx + beta\n",
    "    term2 = -(eta + 1 / tau) * yTx + (1 - 1 / tau) * beta\n",
    "    loss_expr = cp.maximum(term1, term2)\n",
    "    prob = cp.Problem(cp.Minimize(loss_expr))\n",
    "    prob.solve(solver=cp.MOSEK, verbose=False)\n",
    "    return prob.value\n",
    "\n",
    "def oos_mean_portfolio(x, xi):\n",
    "    x = np.asarray(x).reshape(-1)\n",
    "    xi = np.asarray(xi).reshape(-1)\n",
    "    return float(x @ xi)\n",
    "\n",
    "def oos_CVaR_portfolio(x, xi, tau):\n",
    "    portfolio_returns = xi @ x\n",
    "    var_level = np.quantile(portfolio_returns, tau)\n",
    "    cvar = portfolio_returns[portfolio_returns <= var_level].mean()\n",
    "    return -cvar\n",
    "\n",
    "def oos_std_portfolio(x, xi):\n",
    "    portfolio_returns = xi @ x\n",
    "    return np.std(portfolio_returns)\n",
    "\n",
    "def oos_sharpe_portfolio(x, xi):\n",
    "    returns = xi @ x\n",
    "    mean_ret = np.mean(returns)\n",
    "    std_ret = np.std(returns)\n",
    "    return np.sqrt(252) * (mean_ret / std_ret)\n",
    "\n",
    "def select_K_by_AIC(z_np, max_K):\n",
    "    aic_scores = []\n",
    "    models = []\n",
    "    for k in range(1, max_K + 1):\n",
    "        gmm = GaussianMixture(n_components=k)\n",
    "        gmm.fit(z_np)\n",
    "        aic = gmm.aic(z_np)\n",
    "        aic_scores.append(aic)\n",
    "        models.append(gmm)\n",
    "    best_index = np.argmin(aic_scores)\n",
    "    best_K = best_index + 1\n",
    "    return best_K\n",
    "\n",
    "\n",
    "def _cv_gmm_worker(\n",
    "    j, asset_idx,\n",
    "    tau, eta,\n",
    "    data_cv_train, data_cv_test,\n",
    "    eps_list, max_K, hidden_node, hidden_layer, block_size, num_bins, total_epoch,\n",
    "    device\n",
    "):\n",
    "    base = 5000\n",
    "    random.seed(base + j)\n",
    "    np.random.seed(base + j)\n",
    "    torch.manual_seed(base + j)\n",
    "    torch.cuda.manual_seed_all(base + j)  # if using CUDA\n",
    "\n",
    "    # ====== 아래는 기존 로직 유지 ======\n",
    "    dim_s, dim_xi = 5, 399\n",
    "\n",
    "    data_val = data_cv_test.iloc[j]\n",
    "    time_val = data_val['time']\n",
    "    start_time = time_val - pd.DateOffset(years=2)\n",
    "\n",
    "    mask_2year = (data_cv_train['time'] >= start_time) & (data_cv_train['time'] < time_val)\n",
    "    data_subtrain_all = data_cv_train[mask_2year]\n",
    "    s_subtrain = data_subtrain_all.iloc[:, 1:6].values \n",
    "    xi_subtrain = data_subtrain_all.iloc[:, 6 + asset_idx].to_numpy()\n",
    "\n",
    "    s_val = data_val.iloc[1:6].values.reshape(1, -1)\n",
    "    mask_future = (data_cv_train[\"time\"] >= time_val)\n",
    "    future_row = data_cv_train[mask_future]\n",
    "    xi_val_day = future_row.iloc[0, 6 + asset_idx].values.reshape(1, -1)\n",
    "\n",
    "    scaler_s = StandardScaler()\n",
    "    scaler_xi = StandardScaler()\n",
    "    s_subtrain_std = scaler_s.fit_transform(s_subtrain)\n",
    "    xi_subtrain_std = scaler_xi.fit_transform(xi_subtrain)\n",
    "    data_subtrain_std = np.hstack([s_subtrain_std, xi_subtrain_std])\n",
    "    data_subtrain_tensor = torch.tensor(data_subtrain_std, dtype=torch.float32, device=device)\n",
    "\n",
    "    best_K = select_K_by_AIC(data_subtrain_std, max_K=max_K)\n",
    "    latent_size = dim_s + dim_xi\n",
    "\n",
    "    nfm, _ = train_nf_model(\n",
    "        latent_size, best_K, hidden_node, hidden_layer,\n",
    "        num_bins, block_size, total_epoch,\n",
    "        data_subtrain_tensor, device\n",
    "    )\n",
    "\n",
    "    gmm_x = GaussianMixture(\n",
    "        n_components=best_K, covariance_type='diag',\n",
    "        reg_covar=1e-2, random_state = base + j\n",
    "    ).fit(data_subtrain_std)\n",
    "    mu_x, diag_sig_x, p_x = gmm_x.means_, gmm_x.covariances_, gmm_x.weights_\n",
    "    sig_x = np.array([np.diag(diag_sig_x[k]) for k in range(best_K)])\n",
    "\n",
    "    s_val_std = scaler_s.transform(s_val)\n",
    "    s_vec = s_val_std.ravel()\n",
    "\n",
    "    mu_cond_x, cov_cond_x, w_x = transforming_conditional(\n",
    "        s=s_vec, num_components=best_K, mu_k=mu_x, sig_k=sig_x, p_k=p_x, dim_s=dim_s\n",
    "    )\n",
    "    xi_hat_std = (w_x[:, None] * mu_cond_x).sum(axis=0, keepdims=True)\n",
    "\n",
    "    s_aug_std = np.hstack([s_val_std, xi_hat_std])\n",
    "    s_tensor = torch.tensor(s_aug_std, dtype=torch.float32, device=device)\n",
    "\n",
    "    z_s = inverse(nfm, s_tensor)[:, :dim_s][0]\n",
    "    z_train = inverse(nfm, data_subtrain_tensor)\n",
    "\n",
    "    gmm_z = GaussianMixture(\n",
    "        n_components=best_K, covariance_type='diag',\n",
    "        reg_covar=1e-2, random_state = base + j\n",
    "    ).fit(z_train)\n",
    "    mu_k, diag_sig_k, p_k = gmm_z.means_, gmm_z.covariances_, gmm_z.weights_\n",
    "    sig_k = np.array([np.diag(diag_sig_k[k]) for k in range(best_K)])\n",
    "\n",
    "    mu_cond, cov_cond, p_cond = transforming_conditional(\n",
    "        z_s, best_K, mu_k, sig_k, p_k, dim_s\n",
    "    )\n",
    "\n",
    "    z_xi_sample = MC_sampling(best_K, 1000, mu_cond, cov_cond, p_cond)\n",
    "    z_full = np.hstack([np.repeat(z_s.reshape(1, -1), len(z_xi_sample), axis=0), z_xi_sample])\n",
    "\n",
    "    z_tensor = torch.tensor(z_full, dtype=torch.float32, device=device)\n",
    "    x_gen_std = forward(nfm, z_tensor)\n",
    "    xi_MC = scaler_xi.inverse_transform(x_gen_std[:, dim_s:])\n",
    "\n",
    "    eps_losses = {}\n",
    "    for eps in eps_list:\n",
    "        x_cv_gmm = Portfolio_2_Wass_MCVaR(xi_MC, eps, tau, eta)\n",
    "        losses = oos_loss_valid(x_cv_gmm, xi_val_day, tau, eta) * 100\n",
    "        eps_losses[eps] = float(np.mean(losses))\n",
    "        print(f\"[GMM-CV] j={j} eps={eps:.4f}, loss={eps_losses[eps]:.4f}\")\n",
    "\n",
    "    return eps_losses, best_K\n",
    "\n",
    "def cv_GMM(\n",
    "    tau, eta, data_cv_train, data_cv_test, val_indices, val_asset_indices,\n",
    "    eps_list, max_K, hidden_node, hidden_layer, block_size, num_bins, total_epoch,\n",
    "    device, n_jobs=-1\n",
    "):\n",
    "    dim_s, dim_xi = 5, 399\n",
    "    results = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(_cv_gmm_worker)(\n",
    "            int(j),               \n",
    "            asset_idx,            \n",
    "            tau, eta,\n",
    "            data_cv_train, data_cv_test,\n",
    "            eps_list, max_K, hidden_node, hidden_layer, block_size, num_bins, total_epoch,\n",
    "            device\n",
    "        )\n",
    "        for j, asset_idx in zip(val_indices, val_asset_indices)\n",
    "    )\n",
    "\n",
    "    eps_loss_dict = {eps: 0.0 for eps in eps_list}\n",
    "    best_K_list = []\n",
    "    for eps_losses, best_K in results:\n",
    "        best_K_list.append(best_K)\n",
    "        for eps in eps_list:\n",
    "            eps_loss_dict[eps] += eps_losses[eps]\n",
    "\n",
    "    for eps in eps_list:\n",
    "        print(f\"[GMM-CV] eps={eps:.4f}, total_loss={eps_loss_dict[eps]:.4f}\")\n",
    "\n",
    "    best_eps = min(eps_loss_dict.items(), key=lambda x: x[1])[0]\n",
    "    K_mean = float(np.mean(best_K_list))\n",
    "    return best_eps, K_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "15d0ef392bb70140",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T22:54:29.328713Z",
     "start_time": "2025-07-24T22:54:29.321777Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_nf_model(latent_size, best_K, hidden_node, hidden_layer, num_bins, block_size, total_epoch, x, device, batch_size=64, lr=1e-3):\n",
    "    patience = 30\n",
    "    val_split = 0.2\n",
    "\n",
    "    x_np = x.cpu().numpy()\n",
    "    gmm = GaussianMixture(n_components=best_K,covariance_type='diag', reg_covar=1e-3).fit(x_np)\n",
    "\n",
    "    means = torch.tensor(gmm.means_, dtype=torch.float32, device=device)\n",
    "    stds = torch.tensor(np.sqrt(gmm.covariances_), dtype=torch.float32, device=device)\n",
    "    weights = torch.tensor(gmm.weights_, dtype=torch.float32, device=device)\n",
    "\n",
    "    flows = [nf.flows.AutoregressiveRationalQuadraticSpline(latent_size, hidden_layer, hidden_node, num_bins=num_bins) for _ in range(block_size)]\n",
    "\n",
    "    q0 = nf.distributions.GaussianMixture(n_modes=best_K, dim=latent_size, loc=means, scale=stds, weights=weights, trainable=False)\n",
    "    nfm = nf.NormalizingFlow(q0=q0, flows=flows).to(device)\n",
    "    optimizer = torch.optim.Adam(nfm.parameters(), lr=lr)\n",
    "    loss_hist = []\n",
    "\n",
    "    N = x.size(0)\n",
    "    val_size = int(N * val_split)\n",
    "    train_size = N - val_size\n",
    "    train_dataset, val_dataset = random_split(TensorDataset(x), [train_size, val_size])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=val_size, shuffle=False)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(total_epoch):\n",
    "        nfm.train()\n",
    "        train_loss_epoch = 0.0\n",
    "        for batch in train_loader:\n",
    "            x_batch = batch[0].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = nfm.forward_kld(x_batch)\n",
    "            if not torch.isnan(loss):\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss_epoch += loss.item()\n",
    "        loss_hist.append(train_loss_epoch)\n",
    "\n",
    "        nfm.eval()\n",
    "        with torch.no_grad():\n",
    "            for val_batch in val_loader:\n",
    "                x_val = val_batch[0].to(device)\n",
    "                val_loss = nfm.forward_kld(x_val).item()\n",
    "\n",
    "        if val_loss < best_val_loss - 1e-4:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = nfm.state_dict()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"⏹️ Early stopping at epoch {epoch+1}, best val loss: {best_val_loss:.4f}\")\n",
    "                break\n",
    "\n",
    "    if best_model_state is not None:\n",
    "        nfm.load_state_dict(best_model_state)\n",
    "\n",
    "    return nfm, loss_hist\n",
    "\n",
    "def inverse(nfm, x):\n",
    "    with torch.no_grad():\n",
    "        z_np = nfm.inverse(x).cpu().numpy()\n",
    "    return z_np\n",
    "\n",
    "def forward(nfm, z):\n",
    "    with torch.no_grad():\n",
    "        x = nfm.forward(z).cpu().numpy()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cac2d045a46d3356",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T22:54:29.392087Z",
     "start_time": "2025-07-24T22:54:29.384379Z"
    }
   },
   "outputs": [],
   "source": [
    "def equal_weight_kernel(X_mat: np.array,Y_mat: np.array,X0: np.array) -> np.array:\n",
    "    Y_mat = np.asarray(Y_mat, dtype=np.float64)\n",
    "    num_assets = Y_mat.shape[1]\n",
    "    return np.ones(num_assets) / num_assets\n",
    "\n",
    "def mean_CVaR_kernel(X_mat:np.array, Y_mat:np.array, X0:np.array, reg_params:float, tau:float,)->np.array:\n",
    "    Y_mat = np.asarray(Y_mat, dtype=np.float64)\n",
    "\n",
    "    num_sample = Y_mat.shape[0]\n",
    "    dim_beta = Y_mat.shape[1]\n",
    "    alpha = cp.Variable(shape = (1,), name = 'alpha')\n",
    "    beta = cp.Variable(shape = (dim_beta,), name = 'beta', nonneg=True)\n",
    "    lambda_ = cp.Variable(shape = (num_sample,), name = 'lambda')\n",
    "    constraints = [\n",
    "        cp.sum(beta) == 1,\n",
    "        lambda_ >= -reg_params*(Y_mat@beta) + alpha,\n",
    "        lambda_ >= -(reg_params+1/tau)*(Y_mat@beta) + (1-1/tau)*alpha,\n",
    "    ]\n",
    "    problem = cp.Problem(cp.Minimize(cp.sum(lambda_)), constraints)\n",
    "    problem.solve()\n",
    "    if problem.status != 'optimal':\n",
    "        raise ValueError('problem is not optimal')\n",
    "    return beta.value\n",
    "\n",
    "def DR_mean_CVaR_kernel(X_mat: np.array, Y_mat: np.array, X0: np.array, reg_params: float, tau: float, rho: float):\n",
    "    Y_mat = np.asarray(Y_mat, dtype=np.float64)\n",
    "\n",
    "    num_sample = Y_mat.shape[0]\n",
    "    dim_beta = Y_mat.shape[1]\n",
    "    alpha = cp.Variable(shape = (1,), name = 'alpha')\n",
    "    beta = cp.Variable(shape = (dim_beta,), name = 'beta', nonneg=True)\n",
    "    lambda_ = cp.Variable(shape = (1,), name = 'lambda', nonneg=True)\n",
    "    inside_exp = cp.Variable(shape = (num_sample,), name = 'inside_exp')\n",
    "    constraints = [\n",
    "        cp.sum(beta) == 1,\n",
    "        inside_exp >= -reg_params*(Y_mat@beta) + alpha + cp.quad_over_lin(reg_params*beta,4*lambda_),\n",
    "        inside_exp >= (-(reg_params+1/tau)*(Y_mat@beta) +\n",
    "                       (1-1/tau)*alpha + cp.quad_over_lin((reg_params+1/tau)*beta,4*lambda_)),\n",
    "    ]\n",
    "    problem = cp.Problem(cp.Minimize(lambda_*rho + cp.sum(inside_exp)/num_sample), constraints)\n",
    "    problem.solve()\n",
    "    if problem.status != 'optimal':\n",
    "        raise ValueError('problem is not optimal')\n",
    "    return beta.value\n",
    "\n",
    "def cond_mean_CVaR_kernel(X_mat: np.array, Y_mat: np.array, X0: np.array, reg_params: float, tau: float, neighbor_quantile: float):\n",
    "    X_mat = np.asarray(X_mat, dtype=np.float64)\n",
    "    X0 = np.asarray(X0, dtype=np.float64).reshape(1, -1)\n",
    "    Y_mat = np.asarray(Y_mat, dtype=np.float64)\n",
    "\n",
    "    X_dist = np.linalg.norm(X_mat-X0, axis = 1)\n",
    "    idx = (X_dist <= np.quantile(X_dist, neighbor_quantile))\n",
    "    dim_beta = Y_mat.shape[1]\n",
    "    dim_data = np.sum(idx)\n",
    "    alpha = cp.Variable(shape = (1,), name = 'alpha')\n",
    "    beta = cp.Variable(shape = (dim_beta,), name = 'beta', nonneg=True)\n",
    "    lambda_ = cp.Variable(shape = (dim_data,), name = 'lambda')\n",
    "    constraints = [\n",
    "        cp.sum(beta) == 1,\n",
    "        lambda_ >= -reg_params*(Y_mat[idx,:]@beta) + alpha,\n",
    "        lambda_ >= -(reg_params+1/tau)*(Y_mat[idx,:]@beta) + (1-1/tau)*alpha,\n",
    "    ]\n",
    "    problem = cp.Problem(cp.Minimize(cp.sum(lambda_)), constraints)\n",
    "    problem.solve()\n",
    "    if problem.status != 'optimal':\n",
    "        raise ValueError('problem is not optimal')\n",
    "    return beta.value\n",
    "\n",
    "def DR_Winf_conditional_mean_CVaR_kernel(X_mat: np.array, Y_mat: np.array, X0: np.array, reg_params: float, tau: float, gamma_quantile: float, rho_quantile: float):\n",
    "    X_mat = np.asarray(X_mat, dtype=np.float64)\n",
    "    X0 = np.asarray(X0, dtype=np.float64).reshape(1, -1)\n",
    "    Y_mat = np.asarray(Y_mat, dtype=np.float64)\n",
    "\n",
    "    eta = reg_params\n",
    "    tau_inv = 1 / tau\n",
    "    X_dist = np.linalg.norm(X_mat - X0, axis=1)\n",
    "    X_dist[np.isnan(X_dist)] = 1e8\n",
    "    gamma = np.quantile(X_dist, gamma_quantile)\n",
    "    rho = np.quantile(X_dist, rho_quantile)\n",
    "    try:\n",
    "        idx_I = (X_dist <= gamma + rho)\n",
    "        idx_I1 = (X_dist + rho <= gamma)\n",
    "        idx_I2 = idx_I & (~idx_I1)\n",
    "    except RuntimeWarning:\n",
    "        print(X_dist)\n",
    "        print(gamma)\n",
    "        print(rho)\n",
    "    norm_x_minus_xp_in_I = X_dist[idx_I] - gamma\n",
    "    norm_x_minus_xp_in_I[norm_x_minus_xp_in_I < 0] = 0\n",
    "    y_I = Y_mat[idx_I]\n",
    "\n",
    "    stock_num = Y_mat.shape[1]\n",
    "    beta = cp.Variable(stock_num, nonneg=True)\n",
    "    alpha = cp.Variable(1)\n",
    "    lambda_ = cp.Variable(shape=(1,))\n",
    "    u = cp.Variable(shape=(len(y_I),), name='u')\n",
    "    v_term_1 = alpha - eta * (Y_mat[idx_I] @ beta) + eta * cp.norm(beta) * (rho - norm_x_minus_xp_in_I)\n",
    "    v_term_2 = ((1 - tau_inv) * alpha\n",
    "                - (eta + tau_inv) * (Y_mat[idx_I] @ beta)\n",
    "                + (eta + tau_inv) * cp.norm(beta) * (rho - norm_x_minus_xp_in_I))\n",
    "    constraints = [\n",
    "        u[idx_I2[idx_I]] >= 0,\n",
    "        cp.sum(u) <= 0,\n",
    "        cp.sum(beta) == 1,\n",
    "        lambda_ + u >= v_term_1,\n",
    "        lambda_ + u >= v_term_2\n",
    "    ]\n",
    "    problem = cp.Problem(cp.Minimize(lambda_), constraints)\n",
    "    problem.solve()\n",
    "    if problem.status != 'optimal':\n",
    "        raise ValueError('problem is not optimal')\n",
    "    return beta.value\n",
    "\n",
    "def DR_W2_conditional_mean_CVaR_kernel(X_mat: np.array, Y_mat: np.array, X0: np.array, reg_params: float, tau: float, epsilon: float, rho_div_rho_min: float,):\n",
    "    X_mat = np.asarray(X_mat, dtype=np.float64)\n",
    "    X0 = np.asarray(X0, dtype=np.float64).reshape(1, -1)\n",
    "    Y_mat = np.asarray(Y_mat, dtype=np.float64)\n",
    "\n",
    "    def compute_rho_min(X_mat, X0, epsilon):\n",
    "        X_dist = np.linalg.norm(X_mat - X0, axis=1)\n",
    "        X_dist[np.isnan(X_dist)] = 1e8\n",
    "        X_cut = np.quantile(X_dist, q=epsilon, method='higher')\n",
    "        return (X_dist[X_dist <= X_cut]**2).mean() * epsilon\n",
    "\n",
    "    rho = rho_div_rho_min * compute_rho_min(X_mat, X0, epsilon)\n",
    "    X_dist = np.linalg.norm(X_mat - X0, axis=1)\n",
    "    eta = reg_params\n",
    "    epsilon_inv = 1 / epsilon\n",
    "    tau_inv = 1 / tau\n",
    "\n",
    "    N, stock_num = Y_mat.shape\n",
    "    beta = cp.Variable(stock_num, nonneg=True)\n",
    "    alpha = cp.Variable(1)\n",
    "    lambda1 = cp.Variable(1, nonneg=True)\n",
    "    lambda2 = cp.Variable(1)\n",
    "    theta = cp.Variable(N, nonneg=True)\n",
    "    z = cp.Variable(N, nonneg=True)\n",
    "    z_tilde = cp.Variable(N, nonneg=True)\n",
    "\n",
    "    obj = cp.Minimize(lambda1 * rho + lambda2 * epsilon + cp.sum(theta) / N)\n",
    "    linear_constraints = [\n",
    "        cp.sum(beta) == 1,\n",
    "        z == theta + lambda1 * X_dist ** 2 + lambda2 + epsilon_inv * eta * (Y_mat @ beta - alpha),\n",
    "        z_tilde == (theta + lambda1 * X_dist ** 2 + lambda2\n",
    "                    + epsilon_inv * (eta + tau_inv) * (Y_mat @ beta)\n",
    "                    - epsilon_inv * (1 - tau_inv) * alpha)\n",
    "    ]\n",
    "    quad_over_lin_constraints = [\n",
    "        z >= cp.quad_over_lin(epsilon_inv * eta * beta, 4 * lambda1),\n",
    "        z_tilde >= cp.quad_over_lin(epsilon_inv * (eta + tau_inv) * beta, 4 * lambda1),\n",
    "    ]\n",
    "    problem = cp.Problem(obj, linear_constraints + quad_over_lin_constraints)\n",
    "    problem.solve()\n",
    "    if problem.status != 'optimal':\n",
    "        raise ValueError('problem is not optimal')\n",
    "    return beta.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aecff89ac28df94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T22:54:29.430432Z",
     "start_time": "2025-07-24T22:54:29.423818Z"
    }
   },
   "outputs": [],
   "source": [
    "def cv_DR_mean_CVaR_kernel(tau, eta, data_cv_train, data_cv_test, val_indices, val_asset_indices, eps_list):\n",
    "    best_eps, best_loss = None, float('inf')\n",
    "    for eps in eps_list:\n",
    "        total_loss = 0\n",
    "        for j, asset_idx in zip(val_indices, val_asset_indices):\n",
    "            data_val = data_cv_test.iloc[j]\n",
    "            time_val = data_val['time']\n",
    "            start_time = time_val - pd.DateOffset(years=2)\n",
    "\n",
    "            mask_2year = (data_cv_train['time'] >= start_time) & (data_cv_train['time'] < time_val)\n",
    "            data_subtrain_all = data_cv_train[mask_2year]\n",
    "            s_subtrain = data_subtrain_all.iloc[:, 1:6].values\n",
    "            xi_subtrain = data_subtrain_all.iloc[:, 6:].values\n",
    "            xi_sub = xi_subtrain[:, asset_idx]\n",
    "\n",
    "            s_val = data_val.iloc[1:6].values.reshape(1, -1)\n",
    "\n",
    "            future_rows = data_cv_train[data_cv_train[\"time\"] > time_val]\n",
    "            xi_val_day = future_rows.iloc[0, 6 + asset_idx].values.reshape(1, -1)\n",
    "\n",
    "            x = DR_mean_CVaR_kernel(s_subtrain, xi_sub, s_val, eta, tau, rho=eps)\n",
    "            losses = oos_loss_valid(x, xi_val_day, tau, eta) * 100\n",
    "            avg_loss = np.mean(losses)\n",
    "            total_loss += avg_loss\n",
    "\n",
    "        print(f\"[DRMC] eps={eps:.4f}, total_loss={total_loss:.4f}\")\n",
    "        if total_loss < best_loss:\n",
    "            best_loss = total_loss\n",
    "            best_eps = eps\n",
    "\n",
    "    return best_eps\n",
    "\n",
    "def cv_cond_mean_CVaR_kernel(tau, eta, data_cv_train, data_cv_test, val_indices, val_asset_indices, quantile_list):\n",
    "    best_q, best_loss = None, float('inf')\n",
    "    for q in quantile_list:\n",
    "        total_loss = 0\n",
    "        for j, asset_idx in zip(val_indices, val_asset_indices):\n",
    "            data_val = data_cv_test.iloc[j]\n",
    "            time_val = data_val['time']\n",
    "            start_time = time_val - pd.DateOffset(years=2)\n",
    "\n",
    "            mask_2year = (data_cv_train['time'] >= start_time) & (data_cv_train['time'] < time_val)\n",
    "            data_subtrain_all = data_cv_train[mask_2year]\n",
    "            s_subtrain = data_subtrain_all.iloc[:, 1:6].values\n",
    "            xi_subtrain = data_subtrain_all.iloc[:, 6:].values\n",
    "            xi_sub = xi_subtrain[:, asset_idx]\n",
    "\n",
    "            s_val = data_val.iloc[1:6].values.reshape(1, -1)\n",
    "\n",
    "            future_rows = data_cv_train[data_cv_train[\"time\"] > time_val]\n",
    "            xi_val_day = future_rows.iloc[0, 6 + asset_idx].values.reshape(1, -1)\n",
    "\n",
    "            x_CMC = cond_mean_CVaR_kernel(s_subtrain, xi_sub, s_val, eta, tau, neighbor_quantile=q)\n",
    "            losses = oos_loss_valid(x_CMC, xi_val_day, tau, eta) * 100\n",
    "            avg_loss = np.mean(losses)\n",
    "            total_loss += avg_loss\n",
    "\n",
    "        print(f\"[CMC] quantile={q:.2f}, loss={total_loss:.4f}\")\n",
    "        if total_loss < best_loss:\n",
    "            best_loss, best_q = total_loss, q\n",
    "\n",
    "    return best_q\n",
    "\n",
    "def cv_DR_Winf_conditional_mean_CVaR_kernel(tau, eta, data_cv_train, data_cv_test, val_indices, val_asset_indices, gamma_quantile_list, rho_quantile_list):\n",
    "    best_loss, best_gamma_q, best_rho_q = float('inf'), None, None\n",
    "    for gamma_q in gamma_quantile_list:\n",
    "        for rho_q in rho_quantile_list:\n",
    "            total_loss = 0\n",
    "\n",
    "            for j, asset_idx in zip(val_indices, val_asset_indices):\n",
    "                data_val = data_cv_test.iloc[j]\n",
    "                time_val = data_val['time']\n",
    "                start_time = time_val - pd.DateOffset(years=2)\n",
    "\n",
    "                mask_2year = (data_cv_train['time'] >= start_time) & (data_cv_train['time'] < time_val)\n",
    "                data_subtrain_all = data_cv_train[mask_2year]\n",
    "                s_subtrain = data_subtrain_all.iloc[:, 1:6].values\n",
    "                xi_subtrain = data_subtrain_all.iloc[:, 6:].values\n",
    "                xi_sub = xi_subtrain[:, asset_idx]\n",
    "\n",
    "                s_val = data_val.iloc[1:6].values.reshape(1, -1)\n",
    "                future_rows = data_cv_train[data_cv_train['time'] > time_val]\n",
    "                xi_val_day = future_rows.iloc[0, 6 + asset_idx].values.reshape(1, -1)\n",
    "\n",
    "                x_DRCMC = DR_Winf_conditional_mean_CVaR_kernel(s_subtrain, xi_sub, s_val, eta, tau, gamma_q, rho_q)\n",
    "                losses = oos_loss_valid(x_DRCMC, xi_val_day, tau, eta) * 100\n",
    "                avg_loss = np.mean(losses)\n",
    "                total_loss += avg_loss\n",
    "\n",
    "            print(f\"[DRCMC] gamma={gamma_q:.2f}, rho={rho_q:.2f}, loss={total_loss:.4f}\")\n",
    "\n",
    "            if total_loss < best_loss:\n",
    "                best_loss = total_loss\n",
    "                best_gamma_q = gamma_q\n",
    "                best_rho_q = rho_q\n",
    "\n",
    "    return best_gamma_q, best_rho_q\n",
    "\n",
    "def cv_DR_W2_conditional_mean_kernel(tau, eta, data_cv_train, data_cv_test, val_indices, val_asset_indices, quantile_level_list, rho_div_rho_min_list):\n",
    "    best_loss, best_quantile_level, best_rho_div = float('inf'), None, None\n",
    "    for q in quantile_level_list:\n",
    "        for rho_div in rho_div_rho_min_list:\n",
    "            total_loss = 0\n",
    "            for j, asset_idx in zip(val_indices, val_asset_indices):\n",
    "                data_val = data_cv_test.iloc[j]\n",
    "                time_val = data_val['time']\n",
    "                start_time = time_val - pd.DateOffset(years=2)\n",
    "\n",
    "                mask_2year = (data_cv_train['time'] >= start_time) & (data_cv_train['time'] < time_val)\n",
    "                data_subtrain_all = data_cv_train[mask_2year]\n",
    "                s_subtrain = data_subtrain_all.iloc[:, 1:6].values\n",
    "                xi_subtrain = data_subtrain_all.iloc[:, 6:].values\n",
    "                xi_sub = xi_subtrain[:, asset_idx]\n",
    "\n",
    "                s_val = data_val.iloc[1:6].values.reshape(1, -1)\n",
    "                future_rows = data_cv_train[data_cv_train['time'] > time_val]\n",
    "                xi_val_day = future_rows.iloc[0, 6 + asset_idx].values.reshape(1, -1)\n",
    "\n",
    "                x_OCTMC = DR_W2_conditional_mean_CVaR_kernel(s_subtrain, xi_sub, s_val, eta, tau, q, rho_div)\n",
    "                losses = oos_loss_valid(x_OCTMC, xi_val_day, tau, eta) * 100\n",
    "                avg_loss = np.mean(losses)\n",
    "                total_loss += avg_loss\n",
    "            print(f\"[OTCMC] quantile={q:.4f}, rho/rho_min={rho_div:.2f}, loss={total_loss:.4f}\")\n",
    "\n",
    "            if total_loss < best_loss:\n",
    "                best_loss = total_loss\n",
    "                best_quantile_level = q\n",
    "                best_rho_div = rho_div\n",
    "\n",
    "    return best_quantile_level, best_rho_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f4025213960cb2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T22:54:29.463195Z",
     "start_time": "2025-07-24T22:54:29.460272Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_avg_return(xi_mat):\n",
    "    return np.mean(xi_mat, axis=1)\n",
    "\n",
    "def NW_weights(x, X_train, h):\n",
    "    dists = np.linalg.norm(X_train - x, axis=1)\n",
    "    weights = np.exp(-0.5 * (dists / h)**2)\n",
    "    return weights / np.sum(weights)\n",
    "\n",
    "def LSCV_bandwidth(x_train, y_train, bandwidths):\n",
    "    n = len(x_train)\n",
    "    errors = []\n",
    "    for h in bandwidths:\n",
    "        total_error = 0\n",
    "        for i in range(n):\n",
    "            x_i = x_train[i]\n",
    "            y_i = y_train[i]\n",
    "            X_rest = np.delete(x_train, i, axis=0)\n",
    "            y_rest = np.delete(y_train, i)\n",
    "            w = NW_weights(x_i, X_rest, h)\n",
    "            y_hat = np.sum(w * y_rest)\n",
    "            total_error += (y_i - y_hat) ** 2\n",
    "        errors.append(total_error)\n",
    "    best_h = bandwidths[np.argmin(errors)]\n",
    "    return best_h\n",
    "\n",
    "def preprocess_side_info(s, xi, bandwidth_candidates=None):\n",
    "    s= np.asarray(s)\n",
    "    if bandwidth_candidates is None:\n",
    "        bandwidth_candidates = np.logspace(-2, 1, 20)\n",
    "\n",
    "    y = compute_avg_return(xi)  # (T,) shape\n",
    "    s_scaled = s.copy()\n",
    "    h_list = []\n",
    "\n",
    "    for j in range(s.shape[1]):\n",
    "        x_j = s[:, j].reshape(-1, 1)\n",
    "        h_j = LSCV_bandwidth(x_j, y, bandwidth_candidates)\n",
    "        h_list.append(h_j)\n",
    "        s_scaled[:, j] = s[:, j] / h_j\n",
    "        print(f\"Side info {j}: selected bandwidth h = {h_j:.4f}\")\n",
    "\n",
    "    return s_scaled, h_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "08c8af56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_project_root():\n",
    "    if '__file__' in globals():\n",
    "        return Path(__file__).resolve().parents[1]\n",
    "\n",
    "    try:\n",
    "        out = subprocess.check_output(['git', 'rev-parse', '--show-toplevel'], text=True).strip()\n",
    "        return Path(out)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    cur = Path.cwd()\n",
    "    for p in [cur] + list(cur.parents):\n",
    "        if (p / 'data').exists():\n",
    "            return p\n",
    "    return cur  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "23d6978cdde91524",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T22:54:29.496831Z",
     "start_time": "2025-07-24T22:54:29.490388Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_trial(j, tau, eta, data_oos_train_set, data_oos_test,\n",
    "              best_eps_DRMC, best_quantile_CMC, best_gamma_q_DRCMC, best_eps_DRCMC,\n",
    "              best_quantile_OTCMC, best_eps_OTCMC, best_eps_GMM, max_K,\n",
    "              hidden_node, hidden_layer, block_size, bins, total_epoch, device):\n",
    "    base = 1000\n",
    "    random.seed(base + j)\n",
    "    np.random.seed(base + j)\n",
    "    torch.manual_seed(base + j)\n",
    "    torch.cuda.manual_seed_all(base + j)  # if using CUDA\n",
    "    dim_s, dim_xi = 5, 399\n",
    "    method_names = [\"GMM\", \"EW\", \"MC\", \"DRMC\", \"CMC\", \"DRCMC\", \"OTCMC\"]\n",
    "    metrics = [\"loss\", \"mean\"]\n",
    "    results = {metric: {method: None for method in method_names} for metric in metrics}\n",
    "    results[\"Trial\"] = j\n",
    "    results[\"K_GMM\"] = None\n",
    "    \n",
    "\n",
    "    # === Extract test sample ===\n",
    "    data_val = data_oos_test.iloc[j]\n",
    "    time_val = data_val[\"time\"]\n",
    "    s_val = data_val.iloc[1:6].values.reshape(1, -1)\n",
    "\n",
    "    # === Select asset indices ===\n",
    "    asset_idx = np.random.choice(399, size=399, replace=False)\n",
    "\n",
    "    # === Get xi_{t+1} (당일 평가용) ===\n",
    "    future_rows = data_oos_train_set[data_oos_train_set[\"time\"] > time_val]\n",
    "    xi_val_day = future_rows.iloc[0, 6 + asset_idx].values.reshape(1, -1)\n",
    "\n",
    "    # === Training data from [t-2y, t) ===\n",
    "    start_time = time_val - pd.DateOffset(years=2)\n",
    "    mask_train = (data_oos_train_set[\"time\"] >= start_time) & (data_oos_train_set[\"time\"] < time_val)\n",
    "    data_train = data_oos_train_set[mask_train]\n",
    "    s_train = data_train.iloc[:, 1:6].values\n",
    "    xi_train = data_train.iloc[:, 6 + asset_idx].values\n",
    "\n",
    "    # === Classical models ===\n",
    "    classical_models = {\n",
    "        \"EW\":  lambda: equal_weight_kernel(s_train, xi_train, s_val),\n",
    "        \"MC\":  lambda: mean_CVaR_kernel(s_train, xi_train, s_val, eta, tau),\n",
    "        \"DRMC\": lambda: DR_mean_CVaR_kernel(s_train, xi_train, s_val, eta, tau, best_eps_DRMC),\n",
    "        \"CMC\": lambda: cond_mean_CVaR_kernel(s_train, xi_train, s_val, eta, tau, best_quantile_CMC),\n",
    "        \"DRCMC\": lambda: DR_Winf_conditional_mean_CVaR_kernel(s_train, xi_train, s_val, eta, tau, best_gamma_q_DRCMC, best_eps_DRCMC),\n",
    "        \"OTCMC\": lambda: DR_W2_conditional_mean_CVaR_kernel(s_train, xi_train, s_val, eta, tau, best_quantile_OTCMC, best_eps_OTCMC),\n",
    "    }\n",
    "\n",
    "    for name, model_func in classical_models.items():\n",
    "        try:\n",
    "            x = model_func()\n",
    "            losses = oos_loss_portfolio(x, xi_val_day, tau, eta)\n",
    "            means = oos_mean_portfolio(x, xi_val_day)\n",
    "            results[\"loss\"][name] = np.mean(losses) * 100\n",
    "            results[\"mean\"][name] = np.mean(means) * 100\n",
    "        except Exception as e:\n",
    "            print(f\"[Trial {j}] Error in {name}: {e}\")\n",
    "\n",
    "    # === GMM model ===\n",
    "    try:\n",
    "        scaler_s = StandardScaler()\n",
    "        scaler_xi = StandardScaler()\n",
    "        s_train_std = scaler_s.fit_transform(s_train)\n",
    "        xi_train_std = scaler_xi.fit_transform(xi_train)\n",
    "        data_train_std = np.concatenate([s_train_std, xi_train_std], axis=1)\n",
    "        data_train_tensor = torch.tensor(data_train_std, dtype=torch.float32, device=device)\n",
    "\n",
    "        best_K_GMM = select_K_by_AIC(data_train_std, max_K=max_K)\n",
    "        results[\"K_GMM\"] = best_K_GMM\n",
    "\n",
    "        nfm, _ = train_nf_model(5 + 399, best_K_GMM, hidden_node, hidden_layer, bins, block_size, total_epoch, data_train_tensor, device)\n",
    "\n",
    "        gmm_x = GaussianMixture(n_components=best_K_GMM, covariance_type='diag', reg_covar=1e-2, random_state = base + j).fit(data_train_std)\n",
    "        mu_x, diag_sig_x, p_x = gmm_x.means_, gmm_x.covariances_, gmm_x.weights_\n",
    "        sig_x = np.array([np.diag(diag_sig_x[k]) for k in range(best_K_GMM)])\n",
    "\n",
    "        s_val = s_val.reshape(1, -1)\n",
    "        s_val_std = scaler_s.transform(s_val) \n",
    "        s_vec = s_val_std.ravel()\n",
    "\n",
    "        mu_cond_x, cov_cond_x, p_cond_x = transforming_conditional(s=s_vec, num_components=best_K_GMM, mu_k=mu_x, sig_k=sig_x, p_k=p_x, dim_s=dim_s)\n",
    "        xi_hat_std = (p_cond_x[:, None] * mu_cond_x).sum(axis=0, keepdims=True)\n",
    "\n",
    "        gamma_std = np.hstack([s_val_std, xi_hat_std])                      # (1, dim_s + dim_xi)\n",
    "        gamma_tensor = torch.tensor(gamma_std, dtype=torch.float32, device=device)\n",
    "        z_s = inverse(nfm, gamma_tensor)[:, :dim_s][0]                      # (dim_s,)\n",
    "\n",
    "        z_train = inverse(nfm, data_train_tensor)\n",
    "        gmm_z = GaussianMixture(n_components=best_K_GMM, covariance_type='diag', reg_covar=1e-2, random_state = base + j).fit(z_train)\n",
    "        mu_z, diag_sig_z, p_z = gmm_z.means_, gmm_z.covariances_, gmm_z.weights_\n",
    "        sig_z = np.array([np.diag(diag_sig_z[k]) for k in range(best_K_GMM)])\n",
    "\n",
    "        mu_cond_z, cov_cond_z, p_cond_z = transforming_conditional(s=z_s, num_components=best_K_GMM, mu_k=mu_z, sig_k=sig_z, p_k=p_z, dim_s=dim_s)\n",
    "        z_xi_sample = MC_sampling(best_K_GMM, 1000, mu_cond_z, cov_cond_z, p_cond_z)\n",
    "        z_full = np.hstack([np.repeat(z_s.reshape(1, -1), len(z_xi_sample), axis=0), z_xi_sample])\n",
    "\n",
    "        z_tensor = torch.tensor(z_full, dtype=torch.float32, device=device)\n",
    "        x_gen_std = forward(nfm, z_tensor)\n",
    "        xi_MC = scaler_xi.inverse_transform(x_gen_std[:, dim_s:])  # (1000, dim_xi)                 \n",
    "\n",
    "        x_GMM = Portfolio_2_Wass_MCVaR(xi_MC, best_eps_GMM, tau, eta)\n",
    "        results[\"loss\"][\"GMM\"] = np.mean(oos_loss_portfolio(x_GMM, xi_val_day, tau, eta)) * 100\n",
    "        results[\"mean\"][\"GMM\"] = np.mean(oos_mean_portfolio(x_GMM, xi_val_day)) * 100\n",
    "    except Exception as e:\n",
    "        print(f\"[Trial {j}] Error in GMM: {e}\")\n",
    "\n",
    "    # === Flatten results ===\n",
    "    flattened = {\"Trial\": results[\"Trial\"], \"K_GMM\": results[\"K_GMM\"]}\n",
    "    for metric in metrics:\n",
    "        for method in method_names:\n",
    "            flattened[f\"{metric}_{method}\"] = results[metric][method]\n",
    "    return flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "20b93bca6f65a992",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-07-24T22:54:29.532006Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vy/tq2b7sv92b3gx_3q075t61y40000gn/T/ipykernel_44577/1828125045.py:7: RuntimeWarning: invalid value encountered in divide\n",
      "  return weights / np.sum(weights)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Side info 0: selected bandwidth h = 0.0100\n",
      "Side info 1: selected bandwidth h = 6.9519\n",
      "Side info 2: selected bandwidth h = 3.3598\n",
      "Side info 3: selected bandwidth h = 0.2637\n",
      "Side info 4: selected bandwidth h = 10.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/vy/tq2b7sv92b3gx_3q075t61y40000gn/T/ipykernel_44577/2741915393.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mval_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrng_main\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_cv_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mval_asset_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mrng_main\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim_xi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m399\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0mbest_eps_DRMC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv_DR_mean_CVaR_kernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtau\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_cv_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_cv_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_asset_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrho_list_DRMC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\u001b[0m\u001b[0;34m[CV-DRMC Finished] best_eps_DRMC = \u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mbest_eps_DRMC\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mbest_quantile_CMC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv_cond_mean_CVaR_kernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtau\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_cv_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_cv_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_asset_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneighbor_quantile_list_CMC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\u001b[0m\u001b[0;34m[CV_CMC] best_quantile_CMC = \u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mbest_quantile_CMC\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/vy/tq2b7sv92b3gx_3q075t61y40000gn/T/ipykernel_44577/3398212388.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(tau, eta, data_cv_train, data_cv_test, val_indices, val_asset_indices, eps_list)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mfuture_rows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_cv_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata_cv_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"time\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mtime_val\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mxi_val_day\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfuture_rows\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0masset_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDR_mean_CVaR_kernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_subtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxi_sub\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtau\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrho\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moos_loss_valid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxi_val_day\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtau\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mavg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mavg_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/vy/tq2b7sv92b3gx_3q075t61y40000gn/T/ipykernel_44577/3410533852.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(X_mat, Y_mat, X0, reg_params, tau, rho)\u001b[0m\n\u001b[1;32m     37\u001b[0m         inside_exp >= (-(reg_params+1/tau)*(Y_mat@beta) +\n\u001b[1;32m     38\u001b[0m                        \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtau\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquad_over_lin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreg_params\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtau\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlambda_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mproblem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProblem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMinimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlambda_\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mrho\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minside_exp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnum_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstraints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mproblem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mproblem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'optimal'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'problem is not optimal'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/cvxpy/problems/problem.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msolver\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m                 raise ValueError(\n\u001b[1;32m    575\u001b[0m                     \u001b[0;34m\"Cannot specify both 'solver' and 'solver_path'. Please choose one.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_solve_solver_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msolve_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msolver_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msolve_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/cvxpy/problems/problem.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, solver, warm_start, verbose, gp, qcp, requires_grad, enforce_dpp, ignore_dpp, canon_backend, **kwargs)\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1157\u001b[0m         \u001b[0msolver_verbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'solver_verbose'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msolver_verbose\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_NUM_SOLVER_STR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1160\u001b[0;31m         solution = solving_chain.solve_via_data(\n\u001b[0m\u001b[1;32m   1161\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarm_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver_verbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_solve_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/cvxpy/reductions/solvers/solving_chain.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, problem, data, warm_start, verbose, solver_opts)\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0mraw\u001b[0m \u001b[0msolver\u001b[0m \u001b[0msolution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0minformation\u001b[0m \u001b[0mreturned\u001b[0m \u001b[0mby\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msolver\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mthis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnecessarily\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m             \u001b[0ma\u001b[0m \u001b[0mSolution\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         \"\"\"\n\u001b[0;32m--> 524\u001b[0;31m         return self.solver.solve_via_data(data, warm_start, verbose,\n\u001b[0m\u001b[1;32m    525\u001b[0m                                           \u001b[0msolver_opts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproblem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_solver_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/cvxpy/reductions/solvers/conic_solvers/mosek_conif.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, data, warm_start, verbose, solver_opts, solver_cache)\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msave_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwritedata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;31m# Optimize the Mosek Task, and return the result.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0mrescode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrescode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmosek\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrescode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrm_max_time\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             warnings.warn(\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/mosek/__init__.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m  11412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  11413\u001b[0m       \u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrmcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  11414\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mtrmcode\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mmosek\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrescode\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0mIs\u001b[0m \u001b[0meither\u001b[0m \u001b[0mOK\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtermination\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  11415\u001b[0m       \"\"\"\n\u001b[0;32m> 11416\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__optimizetrm__1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/mosek/__init__.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m  11401\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__optimizetrm__1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 11402\u001b[0;31m       \u001b[0m_res_optimizetrm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_retargs_optimizetrm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizetrm__1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m  11403\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0m_res_optimizetrm\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  11404\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_msg_optimizetrm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getlasterror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_res_optimizetrm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  11405\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrescode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_res_optimizetrm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_msg_optimizetrm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ============ EXPERIMENT PARAMETERS ============\n",
    "eps_list = [0.01, 0.05, 0.1, 0.5, 1]\n",
    "max_K = 3\n",
    "tau = 0.1\n",
    "eta_list = [1]\n",
    "hidden_node = 64\n",
    "hidden_layer = 2\n",
    "block_size =  2 \n",
    "bins = 8\n",
    "total_epoch = 500\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "rho_list_DRMC = [0.05,0.1,0.25]\n",
    "neighbor_quantile_list_CMC = [0.05,0.1,0.25]\n",
    "gamma_quantile_list_DRCMC = [0.05,0.1,0.25]\n",
    "rho_quantile_list_DRCMC = [0.05,0.1,0.25]\n",
    "quantile_level_list_OTCMC = [0.05, 0.1,0.15]\n",
    "rho_div_rho_min_list_OTCMC = [1.05,1.1,1.15]\n",
    "\n",
    "file_path = \"data/Portfolio_data.csv\"\n",
    "\n",
    "base_seed = 0\n",
    "rng_main = np.random.default_rng(base_seed)\n",
    "\n",
    "\n",
    "for eta in eta_list:\n",
    "    # === Load and parse dataset ===\n",
    "    data = pd.read_csv(file_path)\n",
    "    dim_s, dim_xi, N = 5, 399, data.shape[0]\n",
    "\n",
    "    data[\"time\"] = pd.to_datetime(data[\"time\"])\n",
    "    data.iloc[:, 1:] = data.iloc[:, 1:].astype(np.float64)\n",
    "\n",
    "    # === Preprocess for bandwidth estimation (2017–2020만 사용) ===\n",
    "    mask_scale = (data[\"time\"] >= \"2017-01-01\") & (data[\"time\"] <= \"2020-12-31\")\n",
    "    data_pre = data[mask_scale]\n",
    "    s = data_pre.iloc[:, 1:6]\n",
    "    xi = data_pre.iloc[:, 6:]\n",
    "    _, h_list = preprocess_side_info(s, xi)\n",
    "\n",
    "    # === Scale side info and returns ===\n",
    "    scaled_s = s.values / np.array(h_list)\n",
    "    data_scaled = data.copy()\n",
    "    data_scaled.loc[data_pre.index, data_scaled.columns[1:6]] = scaled_s.astype(float)\n",
    "    data_scaled.iloc[:, 6:] = data_scaled.iloc[:, 6:] / 100\n",
    "\n",
    "    # === Define CV splits ===\n",
    "    mask_cv_train = (data_scaled[\"time\"] >= \"2017-01-01\") & (data_scaled[\"time\"] <= \"2020-12-31\")\n",
    "    mask_cv_test  = (data_scaled[\"time\"] >= \"2019-01-01\") & (data_scaled[\"time\"] <= \"2020-12-31\")\n",
    "\n",
    "    data_cv_train = data_scaled[mask_cv_train]\n",
    "    data_cv_test  = data_scaled[mask_cv_test]\n",
    "\n",
    "\n",
    "    val_indices = rng_main.choice(len(data_cv_test), size=50, replace=False)\n",
    "    val_asset_indices = [rng_main.choice(dim_xi, size=399, replace=False) for _ in val_indices]\n",
    "\n",
    "    best_eps_DRMC = cv_DR_mean_CVaR_kernel(tau, eta, data_cv_train, data_cv_test, val_indices, val_asset_indices, rho_list_DRMC)\n",
    "    print(f\"[CV-DRMC Finished] best_eps_DRMC = {best_eps_DRMC}\")\n",
    "    best_quantile_CMC = cv_cond_mean_CVaR_kernel(tau, eta, data_cv_train, data_cv_test, val_indices, val_asset_indices, neighbor_quantile_list_CMC)\n",
    "    print(f\"[CV_CMC] best_quantile_CMC = {best_quantile_CMC}\")\n",
    "    best_gamma_q_DRCMC, best_eps_DRCMC = cv_DR_Winf_conditional_mean_CVaR_kernel(\n",
    "        tau, eta, data_cv_train, data_cv_test, val_indices, val_asset_indices,\n",
    "        gamma_quantile_list_DRCMC, rho_quantile_list_DRCMC\n",
    "    )\n",
    "    print(f\"[CV-DRCMC] best_gamma_q_DRCMC = {best_gamma_q_DRCMC}, best_eps_DRCMC = {best_eps_DRCMC}\")\n",
    "    best_quantile_OTCMC, best_eps_OTCMC = cv_DR_W2_conditional_mean_kernel(\n",
    "        tau, eta, data_cv_train, data_cv_test, val_indices, val_asset_indices,\n",
    "        quantile_level_list_OTCMC, rho_div_rho_min_list_OTCMC\n",
    "    )\n",
    "    print(f\"[CV-OTCMC] best_quantile_OTCMC = {best_quantile_OTCMC}, best_eps_OTCMC = {best_eps_OTCMC}\")\n",
    "    best_eps_GMM, K_cv_mean = cv_GMM(\n",
    "        tau, eta, data_cv_train, data_cv_test, val_indices, val_asset_indices,\n",
    "        eps_list, max_K, hidden_node, hidden_layer, block_size,\n",
    "        bins, total_epoch, device,\n",
    "        n_jobs=-1          \n",
    "    )\n",
    "    print(f\"[GMM-CV Finished] K={K_cv_mean}, eps={best_eps_GMM}\")\n",
    "\n",
    "    # === Define OOS train and test sets ===\n",
    "    mask_oos_train = (data_scaled[\"time\"] >= \"2019-01-01\") & (data_scaled[\"time\"] <= \"2022-12-31\")\n",
    "    data_oos_train_set = data_scaled[mask_oos_train]\n",
    "\n",
    "    # === Define OOS test set directly ===\n",
    "    mask_oos_test = (data_scaled[\"time\"] >= \"2021-01-01\") & (data_scaled[\"time\"] <= \"2021-12-31\")\n",
    "    data_oos_test = data_scaled[mask_oos_test].reset_index(drop=True)\n",
    "    val_indices = list(range(len(data_oos_test)))\n",
    "\n",
    "    # === Run trials in parallel ===\n",
    "    results = Parallel(n_jobs=-1)(   \n",
    "        delayed(run_trial)(\n",
    "            j, tau, eta,\n",
    "            data_oos_train_set, data_oos_test,\n",
    "            best_eps_DRMC, best_quantile_CMC, best_gamma_q_DRCMC, best_eps_DRCMC,\n",
    "            best_quantile_OTCMC, best_eps_OTCMC, best_eps_GMM, max_K,\n",
    "            hidden_node, hidden_layer, block_size, bins, total_epoch,\n",
    "            device\n",
    "        )\n",
    "        for j in tqdm(val_indices)\n",
    "    )\n",
    "\n",
    "    # --- Clean up / Save ---\n",
    "    results_cleaned = [r for r in results if isinstance(r, dict)]\n",
    "\n",
    "    if len(results_cleaned) == 0:\n",
    "        print(\"⚠️ No valid results; skipping save.\")\n",
    "        continue\n",
    "\n",
    "    df = pd.DataFrame(results_cleaned)\n",
    "\n",
    "    if 'Trial' not in df.columns:\n",
    "        df.insert(0, 'Trial', list(range(len(df))))  \n",
    "\n",
    "    cols = ['Trial'] + [c for c in df.columns if c != 'Trial']\n",
    "    df = df[cols]\n",
    "\n",
    "    mean_row = {'Trial': 'AVG'}\n",
    "    for col in df.columns:\n",
    "        if col != 'Trial':\n",
    "            mean_row[col] = pd.to_numeric(df[col], errors='coerce').mean(skipna=True)\n",
    "    df = pd.concat([df, pd.DataFrame([mean_row])], ignore_index=True)\n",
    "\n",
    "    save_path = f\"PF_full_eta{eta}.csv\"\n",
    "    df.to_csv(save_path, index=False)\n",
    "    print(f\"Saved to {save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
