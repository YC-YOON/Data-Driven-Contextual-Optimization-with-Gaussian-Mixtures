{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a183d196053ba7c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T22:54:29.243180Z",
     "start_time": "2025-07-24T22:54:29.239663Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.core.missing import validate_limit_direction\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "from sklearn.neighbors import KernelDensity\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import multivariate_normal\n",
    "from scipy.stats import truncnorm\n",
    "from tqdm import tqdm\n",
    "import cvxpy as cp\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "from joblib import Parallel, delayed\n",
    "import torch\n",
    "import normflows as nf\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "np.set_printoptions(linewidth=200)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32c107e960973565",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T22:54:29.269985Z",
     "start_time": "2025-07-24T22:54:29.265922Z"
    }
   },
   "outputs": [],
   "source": [
    "# ===================== Deterministic Setup =====================\n",
    "def set_all_seeds(seed: int = 42):\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":16:8\"  \n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    try:\n",
    "        torch.use_deterministic_algorithms(True)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"]=\"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"]=\"1\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"]=\"1\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"]=\"1\"\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":16:8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d00eb2d5f1b4a408",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T22:54:29.300981Z",
     "start_time": "2025-07-24T22:54:29.289514Z"
    }
   },
   "outputs": [],
   "source": [
    "# -- Implementation 3 ---\n",
    "######################### 2-Wass GMM DRO function ###################################\n",
    "def Portfolio_2_Wass(xi, eps):\n",
    "    N, d = xi.shape\n",
    "    lda = cp.Variable(nonneg=True)\n",
    "    s = cp.Variable(N)\n",
    "    x = cp.Variable(d, nonneg=True)\n",
    "\n",
    "    constraints = []\n",
    "    for i in range(N):\n",
    "        constraints.append(\n",
    "            s[i] >= cp.quad_over_lin(x, 4 * lda) - xi[i] @ x\n",
    "        )\n",
    "    constraints.append(cp.sum(x) == 1)\n",
    "\n",
    "    obj = cp.Minimize(lda * (eps ** 2) + (1 / N) * cp.sum(s))\n",
    "    prob = cp.Problem(obj, constraints)\n",
    "    prob.solve(solver=cp.MOSEK, verbose=False)\n",
    "    return x.value\n",
    "\n",
    "def Portfolio_2_Wass_MCVaR(xi, eps, tau, eta):\n",
    "    N, d = xi.shape\n",
    "    lda = cp.Variable(nonneg=True)\n",
    "    s = cp.Variable(N)\n",
    "    x = cp.Variable(d, nonneg=True)\n",
    "    beta = cp.Variable()\n",
    "\n",
    "    const = []\n",
    "    for i in range(N):\n",
    "        xi_norm2 = float(np.sum(xi[i]**2))\n",
    "        const.append(cp.norm2(cp.hstack([2 * lda * xi[i] - ((1 / tau) + eta) * x, lda * xi_norm2 + s[i] + (beta/tau) - beta- lda]))\n",
    "                     <= lda * xi_norm2 + s[i] + (beta/tau) - beta + lda)\n",
    "        const.append(cp.norm2(cp.hstack([2 * lda * xi[i] - eta * x, lda * xi_norm2 + s[i] -  beta - lda]))\n",
    "                     <= lda * xi_norm2 + s[i] - beta + lda)\n",
    "        const.append(lda * xi_norm2 + s[i] >= -(beta/tau) + beta)\n",
    "        const.append(lda * xi_norm2 + s[i] >= beta)\n",
    "    const.append(cp.sum(x) == 1)\n",
    "\n",
    "    obj = cp.Minimize(lda * (eps**2) + (1 / N) * cp.sum(s))\n",
    "    prob = cp.Problem(obj, const)\n",
    "    prob.solve(solver=cp.MOSEK)\n",
    "    return x.value\n",
    "\n",
    "def transforming_conditional(s, num_components, mu_k, sig_k, p_k, dim_s):\n",
    "    mu_cond, cov_cond, weights = [], [], []\n",
    "    for k in range(num_components):\n",
    "        mu = mu_k[k]\n",
    "        sigma = sig_k[k]\n",
    "        mu_s = mu[:dim_s]\n",
    "        mu_xi = mu[dim_s:]\n",
    "        sigma_ss = sigma[:dim_s, :dim_s]\n",
    "        sigma_sx = sigma[:dim_s, dim_s:]\n",
    "        sigma_xs = sigma[dim_s:, :dim_s]\n",
    "        sigma_xx = sigma[dim_s:, dim_s:]\n",
    "        sigma_ss_inv = np.linalg.inv(sigma_ss)\n",
    "        cond_mu = mu_xi + sigma_xs @ sigma_ss_inv @ (s - mu_s)\n",
    "        cond_cov = sigma_xx - sigma_xs @ sigma_ss_inv @ sigma_sx\n",
    "        weight = p_k[k] * multivariate_normal.pdf(s, mean=mu_s, cov=sigma_ss)\n",
    "        mu_cond.append(cond_mu)\n",
    "        cov_cond.append(cond_cov)\n",
    "        weights.append(weight)\n",
    "    weights = np.array(weights)\n",
    "    if np.any(np.isnan(weights)) or weights.sum() <= 1e-12:\n",
    "        weights = np.ones_like(weights) / len(weights)\n",
    "    else:\n",
    "        weights /= weights.sum()\n",
    "    return np.array(mu_cond), np.array(cov_cond), weights\n",
    "\n",
    "def MC_sampling(K, N, mu_list, cov_list, p_list):\n",
    "    d = mu_list.shape[1]\n",
    "    samples = np.zeros((N, d))\n",
    "    for i in range(N):\n",
    "        k = np.random.choice(K, p=p_list)\n",
    "        samples[i] = np.random.multivariate_normal(mu_list[k], cov_list[k])\n",
    "    return samples\n",
    "\n",
    "def oos_loss_portfolio(x, xi, tau, eta):\n",
    "    x = x.reshape(-1)\n",
    "    xi = xi.reshape(-1)\n",
    "    beta = cp.Variable()\n",
    "    yTx = xi @ x\n",
    "    term1 = -eta * yTx + beta\n",
    "    term2 = -(eta + 1 / tau) * yTx + (1 - 1 / tau) * beta\n",
    "    loss_expr = cp.maximum(term1, term2)\n",
    "    prob = cp.Problem(cp.Minimize(loss_expr))\n",
    "    prob.solve(solver=cp.MOSEK, verbose=False)\n",
    "    return prob.value\n",
    "\n",
    "def oos_loss_valid(x, xi, tau, eta):\n",
    "    x = x.reshape(-1)\n",
    "    xi = xi.reshape(-1)\n",
    "    beta = cp.Variable()\n",
    "    yTx = xi @ x\n",
    "    term1 = -eta * yTx + beta\n",
    "    term2 = -(eta + 1 / tau) * yTx + (1 - 1 / tau) * beta\n",
    "    loss_expr = cp.maximum(term1, term2)\n",
    "    prob = cp.Problem(cp.Minimize(loss_expr))\n",
    "    prob.solve(solver=cp.MOSEK, verbose=False)\n",
    "    return prob.value\n",
    "\n",
    "def oos_mean_portfolio(x, xi):\n",
    "    x = np.asarray(x).reshape(-1)\n",
    "    xi = np.asarray(xi).reshape(-1)\n",
    "    return float(x @ xi)\n",
    "\n",
    "def oos_CVaR_portfolio(x, xi, tau):\n",
    "    portfolio_returns = xi @ x\n",
    "    var_level = np.quantile(portfolio_returns, tau)\n",
    "    cvar = portfolio_returns[portfolio_returns <= var_level].mean()\n",
    "    return -cvar\n",
    "\n",
    "def oos_std_portfolio(x, xi):\n",
    "    portfolio_returns = xi @ x\n",
    "    return np.std(portfolio_returns)\n",
    "\n",
    "def oos_sharpe_portfolio(x, xi):\n",
    "    returns = xi @ x\n",
    "    mean_ret = np.mean(returns)\n",
    "    std_ret = np.std(returns)\n",
    "    return np.sqrt(252) * (mean_ret / std_ret)\n",
    "\n",
    "def select_K_by_AIC(z_np, max_K):\n",
    "    aic_scores = []\n",
    "    models = []\n",
    "    for k in range(1, max_K + 1):\n",
    "        gmm = GaussianMixture(n_components=k)\n",
    "        gmm.fit(z_np)\n",
    "        aic = gmm.aic(z_np)\n",
    "        aic_scores.append(aic)\n",
    "        models.append(gmm)\n",
    "    best_index = np.argmin(aic_scores)\n",
    "    best_K = best_index + 1\n",
    "    return best_K\n",
    "\n",
    "\n",
    "def _cv_gmm_worker(\n",
    "    j, asset_idx,\n",
    "    tau, eta,\n",
    "    data_cv_train, data_cv_test,\n",
    "    eps_list, max_K, hidden_node, hidden_layer, block_size, num_bins, total_epoch,\n",
    "    device\n",
    "):\n",
    "    base = 5000\n",
    "    random.seed(base + j)\n",
    "    np.random.seed(base + j)\n",
    "    torch.manual_seed(base + j)\n",
    "    torch.cuda.manual_seed_all(base + j)  # if using CUDA\n",
    "\n",
    "    # ====== 아래는 기존 로직 유지 ======\n",
    "    dim_s, dim_xi = 5, 399\n",
    "\n",
    "    data_val = data_cv_test.iloc[j]\n",
    "    time_val = data_val['time']\n",
    "    start_time = time_val - pd.DateOffset(years=2)\n",
    "\n",
    "    mask_2year = (data_cv_train['time'] >= start_time) & (data_cv_train['time'] < time_val)\n",
    "    data_subtrain_all = data_cv_train[mask_2year]\n",
    "    s_subtrain = data_subtrain_all.iloc[:, 1:6].values \n",
    "    xi_subtrain = data_subtrain_all.iloc[:, 6 + asset_idx].to_numpy()\n",
    "\n",
    "    s_val = data_val.iloc[1:6].values.reshape(1, -1)\n",
    "    mask_future = (data_cv_train[\"time\"] >= time_val)\n",
    "    future_row = data_cv_train[mask_future]\n",
    "    xi_val_day = future_row.iloc[0, 6 + asset_idx].values.reshape(1, -1)\n",
    "\n",
    "    scaler_s = StandardScaler()\n",
    "    scaler_xi = StandardScaler()\n",
    "    s_subtrain_std = scaler_s.fit_transform(s_subtrain)\n",
    "    xi_subtrain_std = scaler_xi.fit_transform(xi_subtrain)\n",
    "    data_subtrain_std = np.hstack([s_subtrain_std, xi_subtrain_std])\n",
    "    data_subtrain_tensor = torch.tensor(data_subtrain_std, dtype=torch.float32, device=device)\n",
    "\n",
    "    best_K = select_K_by_AIC(data_subtrain_std, max_K=max_K)\n",
    "    latent_size = dim_s + dim_xi\n",
    "\n",
    "    nfm, _ = train_nf_model(\n",
    "        latent_size, best_K, hidden_node, hidden_layer,\n",
    "        num_bins, block_size, total_epoch,\n",
    "        data_subtrain_tensor, device\n",
    "    )\n",
    "\n",
    "    gmm_x = GaussianMixture(\n",
    "        n_components=best_K, covariance_type='diag',\n",
    "        reg_covar=1e-2, random_state = base + j\n",
    "    ).fit(data_subtrain_std)\n",
    "    mu_x, diag_sig_x, p_x = gmm_x.means_, gmm_x.covariances_, gmm_x.weights_\n",
    "    sig_x = np.array([np.diag(diag_sig_x[k]) for k in range(best_K)])\n",
    "\n",
    "    s_val_std = scaler_s.transform(s_val)\n",
    "    s_vec = s_val_std.ravel()\n",
    "\n",
    "    mu_cond_x, cov_cond_x, w_x = transforming_conditional(\n",
    "        s=s_vec, num_components=best_K, mu_k=mu_x, sig_k=sig_x, p_k=p_x, dim_s=dim_s\n",
    "    )\n",
    "    xi_hat_std = (w_x[:, None] * mu_cond_x).sum(axis=0, keepdims=True)\n",
    "\n",
    "    s_aug_std = np.hstack([s_val_std, xi_hat_std])\n",
    "    s_tensor = torch.tensor(s_aug_std, dtype=torch.float32, device=device)\n",
    "\n",
    "    z_s = inverse(nfm, s_tensor)[:, :dim_s][0]\n",
    "    z_train = inverse(nfm, data_subtrain_tensor)\n",
    "\n",
    "    gmm_z = GaussianMixture(\n",
    "        n_components=best_K, covariance_type='diag',\n",
    "        reg_covar=1e-2, random_state = base + j\n",
    "    ).fit(z_train)\n",
    "    mu_k, diag_sig_k, p_k = gmm_z.means_, gmm_z.covariances_, gmm_z.weights_\n",
    "    sig_k = np.array([np.diag(diag_sig_k[k]) for k in range(best_K)])\n",
    "\n",
    "    mu_cond, cov_cond, p_cond = transforming_conditional(\n",
    "        z_s, best_K, mu_k, sig_k, p_k, dim_s\n",
    "    )\n",
    "\n",
    "    z_xi_sample = MC_sampling(best_K, 1000, mu_cond, cov_cond, p_cond)\n",
    "    z_full = np.hstack([np.repeat(z_s.reshape(1, -1), len(z_xi_sample), axis=0), z_xi_sample])\n",
    "\n",
    "    z_tensor = torch.tensor(z_full, dtype=torch.float32, device=device)\n",
    "    x_gen_std = forward(nfm, z_tensor)\n",
    "    xi_MC = scaler_xi.inverse_transform(x_gen_std[:, dim_s:])\n",
    "\n",
    "    eps_losses = {}\n",
    "    for eps in eps_list:\n",
    "        x_cv_gmm = Portfolio_2_Wass_MCVaR(xi_MC, eps, tau, eta)\n",
    "        losses = oos_loss_valid(x_cv_gmm, xi_val_day, tau, eta) * 100\n",
    "        eps_losses[eps] = float(np.mean(losses))\n",
    "        print(f\"[GMM-CV] j={j} eps={eps:.4f}, loss={eps_losses[eps]:.4f}\")\n",
    "\n",
    "    return eps_losses, best_K\n",
    "\n",
    "def cv_GMM(\n",
    "    tau, eta, data_cv_train, data_cv_test, val_indices, val_asset_indices,\n",
    "    eps_list, max_K, hidden_node, hidden_layer, block_size, num_bins, total_epoch,\n",
    "    device, n_jobs=-1\n",
    "):\n",
    "    dim_s, dim_xi = 5, 399\n",
    "    results = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(_cv_gmm_worker)(\n",
    "            int(j),               \n",
    "            asset_idx,            \n",
    "            tau, eta,\n",
    "            data_cv_train, data_cv_test,\n",
    "            eps_list, max_K, hidden_node, hidden_layer, block_size, num_bins, total_epoch,\n",
    "            device\n",
    "        )\n",
    "        for j, asset_idx in zip(val_indices, val_asset_indices)\n",
    "    )\n",
    "\n",
    "    eps_loss_dict = {eps: 0.0 for eps in eps_list}\n",
    "    best_K_list = []\n",
    "    for eps_losses, best_K in results:\n",
    "        best_K_list.append(best_K)\n",
    "        for eps in eps_list:\n",
    "            eps_loss_dict[eps] += eps_losses[eps]\n",
    "\n",
    "    for eps in eps_list:\n",
    "        print(f\"[GMM-CV] eps={eps:.4f}, total_loss={eps_loss_dict[eps]:.4f}\")\n",
    "\n",
    "    best_eps = min(eps_loss_dict.items(), key=lambda x: x[1])[0]\n",
    "    K_mean = float(np.mean(best_K_list))\n",
    "    return best_eps, K_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15d0ef392bb70140",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T22:54:29.328713Z",
     "start_time": "2025-07-24T22:54:29.321777Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_nf_model(latent_size, best_K, hidden_node, hidden_layer, num_bins, block_size, total_epoch, x, device, batch_size=64, lr=1e-3):\n",
    "    patience = 30\n",
    "    val_split = 0.2\n",
    "\n",
    "    x_np = x.cpu().numpy()\n",
    "    gmm = GaussianMixture(n_components=best_K,covariance_type='diag', reg_covar=1e-3).fit(x_np)\n",
    "\n",
    "    means = torch.tensor(gmm.means_, dtype=torch.float32, device=device)\n",
    "    stds = torch.tensor(np.sqrt(gmm.covariances_), dtype=torch.float32, device=device)\n",
    "    weights = torch.tensor(gmm.weights_, dtype=torch.float32, device=device)\n",
    "\n",
    "    flows = [nf.flows.AutoregressiveRationalQuadraticSpline(latent_size, hidden_layer, hidden_node, num_bins=num_bins) for _ in range(block_size)]\n",
    "\n",
    "    q0 = nf.distributions.GaussianMixture(n_modes=best_K, dim=latent_size, loc=means, scale=stds, weights=weights, trainable=False)\n",
    "    nfm = nf.NormalizingFlow(q0=q0, flows=flows).to(device)\n",
    "    optimizer = torch.optim.Adam(nfm.parameters(), lr=lr)\n",
    "    loss_hist = []\n",
    "\n",
    "    N = x.size(0)\n",
    "    val_size = int(N * val_split)\n",
    "    train_size = N - val_size\n",
    "    train_dataset, val_dataset = random_split(TensorDataset(x), [train_size, val_size])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=val_size, shuffle=False)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(total_epoch):\n",
    "        nfm.train()\n",
    "        train_loss_epoch = 0.0\n",
    "        for batch in train_loader:\n",
    "            x_batch = batch[0].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = nfm.forward_kld(x_batch)\n",
    "            if not torch.isnan(loss):\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss_epoch += loss.item()\n",
    "        loss_hist.append(train_loss_epoch)\n",
    "\n",
    "        nfm.eval()\n",
    "        with torch.no_grad():\n",
    "            for val_batch in val_loader:\n",
    "                x_val = val_batch[0].to(device)\n",
    "                val_loss = nfm.forward_kld(x_val).item()\n",
    "\n",
    "        if val_loss < best_val_loss - 1e-4:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = nfm.state_dict()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"⏹️ Early stopping at epoch {epoch+1}, best val loss: {best_val_loss:.4f}\")\n",
    "                break\n",
    "\n",
    "    if best_model_state is not None:\n",
    "        nfm.load_state_dict(best_model_state)\n",
    "\n",
    "    return nfm, loss_hist\n",
    "\n",
    "def inverse(nfm, x):\n",
    "    with torch.no_grad():\n",
    "        z_np = nfm.inverse(x).cpu().numpy()\n",
    "    return z_np\n",
    "\n",
    "def forward(nfm, z):\n",
    "    with torch.no_grad():\n",
    "        x = nfm.forward(z).cpu().numpy()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cac2d045a46d3356",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T22:54:29.392087Z",
     "start_time": "2025-07-24T22:54:29.384379Z"
    }
   },
   "outputs": [],
   "source": [
    "def equal_weight_kernel(X_mat: np.array,Y_mat: np.array,X0: np.array) -> np.array:\n",
    "    Y_mat = np.asarray(Y_mat, dtype=np.float64)\n",
    "    num_assets = Y_mat.shape[1]\n",
    "    return np.ones(num_assets) / num_assets\n",
    "\n",
    "def mean_CVaR_kernel(X_mat:np.array, Y_mat:np.array, X0:np.array, reg_params:float, tau:float,)->np.array:\n",
    "    Y_mat = np.asarray(Y_mat, dtype=np.float64)\n",
    "\n",
    "    num_sample = Y_mat.shape[0]\n",
    "    dim_beta = Y_mat.shape[1]\n",
    "    alpha = cp.Variable(shape = (1,), name = 'alpha')\n",
    "    beta = cp.Variable(shape = (dim_beta,), name = 'beta', nonneg=True)\n",
    "    lambda_ = cp.Variable(shape = (num_sample,), name = 'lambda')\n",
    "    constraints = [\n",
    "        cp.sum(beta) == 1,\n",
    "        lambda_ >= -reg_params*(Y_mat@beta) + alpha,\n",
    "        lambda_ >= -(reg_params+1/tau)*(Y_mat@beta) + (1-1/tau)*alpha,\n",
    "    ]\n",
    "    problem = cp.Problem(cp.Minimize(cp.sum(lambda_)), constraints)\n",
    "    problem.solve()\n",
    "    if problem.status != 'optimal':\n",
    "        raise ValueError('problem is not optimal')\n",
    "    return beta.value\n",
    "\n",
    "def DR_mean_CVaR_kernel(X_mat: np.array, Y_mat: np.array, X0: np.array, reg_params: float, tau: float, rho: float):\n",
    "    Y_mat = np.asarray(Y_mat, dtype=np.float64)\n",
    "\n",
    "    num_sample = Y_mat.shape[0]\n",
    "    dim_beta = Y_mat.shape[1]\n",
    "    alpha = cp.Variable(shape = (1,), name = 'alpha')\n",
    "    beta = cp.Variable(shape = (dim_beta,), name = 'beta', nonneg=True)\n",
    "    lambda_ = cp.Variable(shape = (1,), name = 'lambda', nonneg=True)\n",
    "    inside_exp = cp.Variable(shape = (num_sample,), name = 'inside_exp')\n",
    "    constraints = [\n",
    "        cp.sum(beta) == 1,\n",
    "        inside_exp >= -reg_params*(Y_mat@beta) + alpha + cp.quad_over_lin(reg_params*beta,4*lambda_),\n",
    "        inside_exp >= (-(reg_params+1/tau)*(Y_mat@beta) +\n",
    "                       (1-1/tau)*alpha + cp.quad_over_lin((reg_params+1/tau)*beta,4*lambda_)),\n",
    "    ]\n",
    "    problem = cp.Problem(cp.Minimize(lambda_*rho + cp.sum(inside_exp)/num_sample), constraints)\n",
    "    problem.solve()\n",
    "    if problem.status != 'optimal':\n",
    "        raise ValueError('problem is not optimal')\n",
    "    return beta.value\n",
    "\n",
    "def cond_mean_CVaR_kernel(X_mat: np.array, Y_mat: np.array, X0: np.array, reg_params: float, tau: float, neighbor_quantile: float):\n",
    "    X_mat = np.asarray(X_mat, dtype=np.float64)\n",
    "    X0 = np.asarray(X0, dtype=np.float64).reshape(1, -1)\n",
    "    Y_mat = np.asarray(Y_mat, dtype=np.float64)\n",
    "\n",
    "    X_dist = np.linalg.norm(X_mat-X0, axis = 1)\n",
    "    idx = (X_dist <= np.quantile(X_dist, neighbor_quantile))\n",
    "    dim_beta = Y_mat.shape[1]\n",
    "    dim_data = np.sum(idx)\n",
    "    alpha = cp.Variable(shape = (1,), name = 'alpha')\n",
    "    beta = cp.Variable(shape = (dim_beta,), name = 'beta', nonneg=True)\n",
    "    lambda_ = cp.Variable(shape = (dim_data,), name = 'lambda')\n",
    "    constraints = [\n",
    "        cp.sum(beta) == 1,\n",
    "        lambda_ >= -reg_params*(Y_mat[idx,:]@beta) + alpha,\n",
    "        lambda_ >= -(reg_params+1/tau)*(Y_mat[idx,:]@beta) + (1-1/tau)*alpha,\n",
    "    ]\n",
    "    problem = cp.Problem(cp.Minimize(cp.sum(lambda_)), constraints)\n",
    "    problem.solve()\n",
    "    if problem.status != 'optimal':\n",
    "        raise ValueError('problem is not optimal')\n",
    "    return beta.value\n",
    "\n",
    "def DR_Winf_conditional_mean_CVaR_kernel(X_mat: np.array, Y_mat: np.array, X0: np.array, reg_params: float, tau: float, gamma_quantile: float, rho_quantile: float):\n",
    "    X_mat = np.asarray(X_mat, dtype=np.float64)\n",
    "    X0 = np.asarray(X0, dtype=np.float64).reshape(1, -1)\n",
    "    Y_mat = np.asarray(Y_mat, dtype=np.float64)\n",
    "\n",
    "    eta = reg_params\n",
    "    tau_inv = 1 / tau\n",
    "    X_dist = np.linalg.norm(X_mat - X0, axis=1)\n",
    "    X_dist[np.isnan(X_dist)] = 1e8\n",
    "    gamma = np.quantile(X_dist, gamma_quantile)\n",
    "    rho = np.quantile(X_dist, rho_quantile)\n",
    "    try:\n",
    "        idx_I = (X_dist <= gamma + rho)\n",
    "        idx_I1 = (X_dist + rho <= gamma)\n",
    "        idx_I2 = idx_I & (~idx_I1)\n",
    "    except RuntimeWarning:\n",
    "        print(X_dist)\n",
    "        print(gamma)\n",
    "        print(rho)\n",
    "    norm_x_minus_xp_in_I = X_dist[idx_I] - gamma\n",
    "    norm_x_minus_xp_in_I[norm_x_minus_xp_in_I < 0] = 0\n",
    "    y_I = Y_mat[idx_I]\n",
    "\n",
    "    stock_num = Y_mat.shape[1]\n",
    "    beta = cp.Variable(stock_num, nonneg=True)\n",
    "    alpha = cp.Variable(1)\n",
    "    lambda_ = cp.Variable(shape=(1,))\n",
    "    u = cp.Variable(shape=(len(y_I),), name='u')\n",
    "    v_term_1 = alpha - eta * (Y_mat[idx_I] @ beta) + eta * cp.norm(beta) * (rho - norm_x_minus_xp_in_I)\n",
    "    v_term_2 = ((1 - tau_inv) * alpha\n",
    "                - (eta + tau_inv) * (Y_mat[idx_I] @ beta)\n",
    "                + (eta + tau_inv) * cp.norm(beta) * (rho - norm_x_minus_xp_in_I))\n",
    "    constraints = [\n",
    "        u[idx_I2[idx_I]] >= 0,\n",
    "        cp.sum(u) <= 0,\n",
    "        cp.sum(beta) == 1,\n",
    "        lambda_ + u >= v_term_1,\n",
    "        lambda_ + u >= v_term_2\n",
    "    ]\n",
    "    problem = cp.Problem(cp.Minimize(lambda_), constraints)\n",
    "    problem.solve()\n",
    "    if problem.status != 'optimal':\n",
    "        raise ValueError('problem is not optimal')\n",
    "    return beta.value\n",
    "\n",
    "def DR_W2_conditional_mean_CVaR_kernel(X_mat: np.array, Y_mat: np.array, X0: np.array, reg_params: float, tau: float, epsilon: float, rho_div_rho_min: float,):\n",
    "    X_mat = np.asarray(X_mat, dtype=np.float64)\n",
    "    X0 = np.asarray(X0, dtype=np.float64).reshape(1, -1)\n",
    "    Y_mat = np.asarray(Y_mat, dtype=np.float64)\n",
    "\n",
    "    def compute_rho_min(X_mat, X0, epsilon):\n",
    "        X_dist = np.linalg.norm(X_mat - X0, axis=1)\n",
    "        X_dist[np.isnan(X_dist)] = 1e8\n",
    "        X_cut = np.quantile(X_dist, q=epsilon, method='higher')\n",
    "        return (X_dist[X_dist <= X_cut]**2).mean() * epsilon\n",
    "\n",
    "    rho = rho_div_rho_min * compute_rho_min(X_mat, X0, epsilon)\n",
    "    X_dist = np.linalg.norm(X_mat - X0, axis=1)\n",
    "    eta = reg_params\n",
    "    epsilon_inv = 1 / epsilon\n",
    "    tau_inv = 1 / tau\n",
    "\n",
    "    N, stock_num = Y_mat.shape\n",
    "    beta = cp.Variable(stock_num, nonneg=True)\n",
    "    alpha = cp.Variable(1)\n",
    "    lambda1 = cp.Variable(1, nonneg=True)\n",
    "    lambda2 = cp.Variable(1)\n",
    "    theta = cp.Variable(N, nonneg=True)\n",
    "    z = cp.Variable(N, nonneg=True)\n",
    "    z_tilde = cp.Variable(N, nonneg=True)\n",
    "\n",
    "    obj = cp.Minimize(lambda1 * rho + lambda2 * epsilon + cp.sum(theta) / N)\n",
    "    linear_constraints = [\n",
    "        cp.sum(beta) == 1,\n",
    "        z == theta + lambda1 * X_dist ** 2 + lambda2 + epsilon_inv * eta * (Y_mat @ beta - alpha),\n",
    "        z_tilde == (theta + lambda1 * X_dist ** 2 + lambda2\n",
    "                    + epsilon_inv * (eta + tau_inv) * (Y_mat @ beta)\n",
    "                    - epsilon_inv * (1 - tau_inv) * alpha)\n",
    "    ]\n",
    "    quad_over_lin_constraints = [\n",
    "        z >= cp.quad_over_lin(epsilon_inv * eta * beta, 4 * lambda1),\n",
    "        z_tilde >= cp.quad_over_lin(epsilon_inv * (eta + tau_inv) * beta, 4 * lambda1),\n",
    "    ]\n",
    "    problem = cp.Problem(obj, linear_constraints + quad_over_lin_constraints)\n",
    "    problem.solve()\n",
    "    if problem.status != 'optimal':\n",
    "        raise ValueError('problem is not optimal')\n",
    "    return beta.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aecff89ac28df94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T22:54:29.430432Z",
     "start_time": "2025-07-24T22:54:29.423818Z"
    }
   },
   "outputs": [],
   "source": [
    "def cv_DR_mean_CVaR_kernel(tau, eta, data_cv_train, data_cv_test, val_indices, val_asset_indices, eps_list):\n",
    "    best_eps, best_loss = None, float('inf')\n",
    "    for eps in eps_list:\n",
    "        total_loss = 0\n",
    "        for j, asset_idx in zip(val_indices, val_asset_indices):\n",
    "            data_val = data_cv_test.iloc[j]\n",
    "            time_val = data_val['time']\n",
    "            start_time = time_val - pd.DateOffset(years=2)\n",
    "\n",
    "            mask_2year = (data_cv_train['time'] >= start_time) & (data_cv_train['time'] < time_val)\n",
    "            data_subtrain_all = data_cv_train[mask_2year]\n",
    "            s_subtrain = data_subtrain_all.iloc[:, 1:6].values\n",
    "            xi_subtrain = data_subtrain_all.iloc[:, 6:].values\n",
    "            xi_sub = xi_subtrain[:, asset_idx]\n",
    "\n",
    "            s_val = data_val.iloc[1:6].values.reshape(1, -1)\n",
    "\n",
    "            future_rows = data_cv_train[data_cv_train[\"time\"] > time_val]\n",
    "            xi_val_day = future_rows.iloc[0, 6 + asset_idx].values.reshape(1, -1)\n",
    "\n",
    "            x = DR_mean_CVaR_kernel(s_subtrain, xi_sub, s_val, eta, tau, rho=eps)\n",
    "            losses = oos_loss_valid(x, xi_val_day, tau, eta) * 100\n",
    "            avg_loss = np.mean(losses)\n",
    "            total_loss += avg_loss\n",
    "\n",
    "        print(f\"[DRMC] eps={eps:.4f}, total_loss={total_loss:.4f}\")\n",
    "        if total_loss < best_loss:\n",
    "            best_loss = total_loss\n",
    "            best_eps = eps\n",
    "\n",
    "    return best_eps\n",
    "\n",
    "def cv_cond_mean_CVaR_kernel(tau, eta, data_cv_train, data_cv_test, val_indices, val_asset_indices, quantile_list):\n",
    "    best_q, best_loss = None, float('inf')\n",
    "    for q in quantile_list:\n",
    "        total_loss = 0\n",
    "        for j, asset_idx in zip(val_indices, val_asset_indices):\n",
    "            data_val = data_cv_test.iloc[j]\n",
    "            time_val = data_val['time']\n",
    "            start_time = time_val - pd.DateOffset(years=2)\n",
    "\n",
    "            mask_2year = (data_cv_train['time'] >= start_time) & (data_cv_train['time'] < time_val)\n",
    "            data_subtrain_all = data_cv_train[mask_2year]\n",
    "            s_subtrain = data_subtrain_all.iloc[:, 1:6].values\n",
    "            xi_subtrain = data_subtrain_all.iloc[:, 6:].values\n",
    "            xi_sub = xi_subtrain[:, asset_idx]\n",
    "\n",
    "            s_val = data_val.iloc[1:6].values.reshape(1, -1)\n",
    "\n",
    "            future_rows = data_cv_train[data_cv_train[\"time\"] > time_val]\n",
    "            xi_val_day = future_rows.iloc[0, 6 + asset_idx].values.reshape(1, -1)\n",
    "\n",
    "            x_CMC = cond_mean_CVaR_kernel(s_subtrain, xi_sub, s_val, eta, tau, neighbor_quantile=q)\n",
    "            losses = oos_loss_valid(x_CMC, xi_val_day, tau, eta) * 100\n",
    "            avg_loss = np.mean(losses)\n",
    "            total_loss += avg_loss\n",
    "\n",
    "        print(f\"[CMC] quantile={q:.2f}, loss={total_loss:.4f}\")\n",
    "        if total_loss < best_loss:\n",
    "            best_loss, best_q = total_loss, q\n",
    "\n",
    "    return best_q\n",
    "\n",
    "def cv_DR_Winf_conditional_mean_CVaR_kernel(tau, eta, data_cv_train, data_cv_test, val_indices, val_asset_indices, gamma_quantile_list, rho_quantile_list):\n",
    "    best_loss, best_gamma_q, best_rho_q = float('inf'), None, None\n",
    "    for gamma_q in gamma_quantile_list:\n",
    "        for rho_q in rho_quantile_list:\n",
    "            total_loss = 0\n",
    "\n",
    "            for j, asset_idx in zip(val_indices, val_asset_indices):\n",
    "                data_val = data_cv_test.iloc[j]\n",
    "                time_val = data_val['time']\n",
    "                start_time = time_val - pd.DateOffset(years=2)\n",
    "\n",
    "                mask_2year = (data_cv_train['time'] >= start_time) & (data_cv_train['time'] < time_val)\n",
    "                data_subtrain_all = data_cv_train[mask_2year]\n",
    "                s_subtrain = data_subtrain_all.iloc[:, 1:6].values\n",
    "                xi_subtrain = data_subtrain_all.iloc[:, 6:].values\n",
    "                xi_sub = xi_subtrain[:, asset_idx]\n",
    "\n",
    "                s_val = data_val.iloc[1:6].values.reshape(1, -1)\n",
    "                future_rows = data_cv_train[data_cv_train['time'] > time_val]\n",
    "                xi_val_day = future_rows.iloc[0, 6 + asset_idx].values.reshape(1, -1)\n",
    "\n",
    "                x_DRCMC = DR_Winf_conditional_mean_CVaR_kernel(s_subtrain, xi_sub, s_val, eta, tau, gamma_q, rho_q)\n",
    "                losses = oos_loss_valid(x_DRCMC, xi_val_day, tau, eta) * 100\n",
    "                avg_loss = np.mean(losses)\n",
    "                total_loss += avg_loss\n",
    "\n",
    "            print(f\"[DRCMC] gamma={gamma_q:.2f}, rho={rho_q:.2f}, loss={total_loss:.4f}\")\n",
    "\n",
    "            if total_loss < best_loss:\n",
    "                best_loss = total_loss\n",
    "                best_gamma_q = gamma_q\n",
    "                best_rho_q = rho_q\n",
    "\n",
    "    return best_gamma_q, best_rho_q\n",
    "\n",
    "def cv_DR_W2_conditional_mean_kernel(tau, eta, data_cv_train, data_cv_test, val_indices, val_asset_indices, quantile_level_list, rho_div_rho_min_list):\n",
    "    best_loss, best_quantile_level, best_rho_div = float('inf'), None, None\n",
    "    for q in quantile_level_list:\n",
    "        for rho_div in rho_div_rho_min_list:\n",
    "            total_loss = 0\n",
    "            for j, asset_idx in zip(val_indices, val_asset_indices):\n",
    "                data_val = data_cv_test.iloc[j]\n",
    "                time_val = data_val['time']\n",
    "                start_time = time_val - pd.DateOffset(years=2)\n",
    "\n",
    "                mask_2year = (data_cv_train['time'] >= start_time) & (data_cv_train['time'] < time_val)\n",
    "                data_subtrain_all = data_cv_train[mask_2year]\n",
    "                s_subtrain = data_subtrain_all.iloc[:, 1:6].values\n",
    "                xi_subtrain = data_subtrain_all.iloc[:, 6:].values\n",
    "                xi_sub = xi_subtrain[:, asset_idx]\n",
    "\n",
    "                s_val = data_val.iloc[1:6].values.reshape(1, -1)\n",
    "                future_rows = data_cv_train[data_cv_train['time'] > time_val]\n",
    "                xi_val_day = future_rows.iloc[0, 6 + asset_idx].values.reshape(1, -1)\n",
    "\n",
    "                x_OCTMC = DR_W2_conditional_mean_CVaR_kernel(s_subtrain, xi_sub, s_val, eta, tau, q, rho_div)\n",
    "                losses = oos_loss_valid(x_OCTMC, xi_val_day, tau, eta) * 100\n",
    "                avg_loss = np.mean(losses)\n",
    "                total_loss += avg_loss\n",
    "            print(f\"[OTCMC] quantile={q:.4f}, rho/rho_min={rho_div:.2f}, loss={total_loss:.4f}\")\n",
    "\n",
    "            if total_loss < best_loss:\n",
    "                best_loss = total_loss\n",
    "                best_quantile_level = q\n",
    "                best_rho_div = rho_div\n",
    "\n",
    "    return best_quantile_level, best_rho_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4025213960cb2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T22:54:29.463195Z",
     "start_time": "2025-07-24T22:54:29.460272Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_avg_return(xi_mat):\n",
    "    return np.mean(xi_mat, axis=1)\n",
    "\n",
    "def NW_weights(x, X_train, h):\n",
    "    dists = np.linalg.norm(X_train - x, axis=1)\n",
    "    weights = np.exp(-0.5 * (dists / h)**2)\n",
    "    return weights / np.sum(weights)\n",
    "\n",
    "def LSCV_bandwidth(x_train, y_train, bandwidths):\n",
    "    n = len(x_train)\n",
    "    errors = []\n",
    "    for h in bandwidths:\n",
    "        total_error = 0\n",
    "        for i in range(n):\n",
    "            x_i = x_train[i]\n",
    "            y_i = y_train[i]\n",
    "            X_rest = np.delete(x_train, i, axis=0)\n",
    "            y_rest = np.delete(y_train, i)\n",
    "            w = NW_weights(x_i, X_rest, h)\n",
    "            y_hat = np.sum(w * y_rest)\n",
    "            total_error += (y_i - y_hat) ** 2\n",
    "        errors.append(total_error)\n",
    "    best_h = bandwidths[np.argmin(errors)]\n",
    "    return best_h\n",
    "\n",
    "def preprocess_side_info(s, xi, bandwidth_candidates=None):\n",
    "    s= np.asarray(s)\n",
    "    if bandwidth_candidates is None:\n",
    "        bandwidth_candidates = np.logspace(-2, 1, 20)\n",
    "\n",
    "    y = compute_avg_return(xi)  # (T,) shape\n",
    "    s_scaled = s.copy()\n",
    "    h_list = []\n",
    "\n",
    "    for j in range(s.shape[1]):\n",
    "        x_j = s[:, j].reshape(-1, 1)\n",
    "        h_j = LSCV_bandwidth(x_j, y, bandwidth_candidates)\n",
    "        h_list.append(h_j)\n",
    "        s_scaled[:, j] = s[:, j] / h_j\n",
    "        print(f\"Side info {j}: selected bandwidth h = {h_j:.4f}\")\n",
    "\n",
    "    return s_scaled, h_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23d6978cdde91524",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T22:54:29.496831Z",
     "start_time": "2025-07-24T22:54:29.490388Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_trial(j, tau, eta, data_oos_train_set, data_oos_test,\n",
    "              best_eps_DRMC, best_quantile_CMC, best_gamma_q_DRCMC, best_eps_DRCMC,\n",
    "              best_quantile_OTCMC, best_eps_OTCMC, best_eps_GMM, max_K,\n",
    "              hidden_node, hidden_layer, block_size, bins, total_epoch, device):\n",
    "    base = 1000\n",
    "    random.seed(base + j)\n",
    "    np.random.seed(base + j)\n",
    "    torch.manual_seed(base + j)\n",
    "    torch.cuda.manual_seed_all(base + j)  # if using CUDA\n",
    "    dim_s, dim_xi = 5, 399\n",
    "    method_names = [\"GMM\", \"EW\", \"MC\", \"DRMC\", \"CMC\", \"DRCMC\", \"OTCMC\"]\n",
    "    metrics = [\"loss\", \"mean\"]\n",
    "    results = {metric: {method: None for method in method_names} for metric in metrics}\n",
    "    results[\"Trial\"] = j\n",
    "    results[\"K_GMM\"] = None\n",
    "    \n",
    "\n",
    "    # === Extract test sample ===\n",
    "    data_val = data_oos_test.iloc[j]\n",
    "    time_val = data_val[\"time\"]\n",
    "    s_val = data_val.iloc[1:6].values.reshape(1, -1)\n",
    "\n",
    "    # === Select asset indices ===\n",
    "    asset_idx = np.random.choice(399, size=399, replace=False)\n",
    "\n",
    "    # === Get xi_{t+1} (당일 평가용) ===\n",
    "    future_rows = data_oos_train_set[data_oos_train_set[\"time\"] > time_val]\n",
    "    xi_val_day = future_rows.iloc[0, 6 + asset_idx].values.reshape(1, -1)\n",
    "\n",
    "    # === Training data from [t-2y, t) ===\n",
    "    start_time = time_val - pd.DateOffset(years=2)\n",
    "    mask_train = (data_oos_train_set[\"time\"] >= start_time) & (data_oos_train_set[\"time\"] < time_val)\n",
    "    data_train = data_oos_train_set[mask_train]\n",
    "    s_train = data_train.iloc[:, 1:6].values\n",
    "    xi_train = data_train.iloc[:, 6 + asset_idx].values\n",
    "\n",
    "    # === Classical models ===\n",
    "    classical_models = {\n",
    "        \"EW\":  lambda: equal_weight_kernel(s_train, xi_train, s_val),\n",
    "        \"MC\":  lambda: mean_CVaR_kernel(s_train, xi_train, s_val, eta, tau),\n",
    "        \"DRMC\": lambda: DR_mean_CVaR_kernel(s_train, xi_train, s_val, eta, tau, best_eps_DRMC),\n",
    "        \"CMC\": lambda: cond_mean_CVaR_kernel(s_train, xi_train, s_val, eta, tau, best_quantile_CMC),\n",
    "        \"DRCMC\": lambda: DR_Winf_conditional_mean_CVaR_kernel(s_train, xi_train, s_val, eta, tau, best_gamma_q_DRCMC, best_eps_DRCMC),\n",
    "        \"OTCMC\": lambda: DR_W2_conditional_mean_CVaR_kernel(s_train, xi_train, s_val, eta, tau, best_quantile_OTCMC, best_eps_OTCMC),\n",
    "    }\n",
    "\n",
    "    for name, model_func in classical_models.items():\n",
    "        try:\n",
    "            x = model_func()\n",
    "            losses = oos_loss_portfolio(x, xi_val_day, tau, eta)\n",
    "            means = oos_mean_portfolio(x, xi_val_day)\n",
    "            results[\"loss\"][name] = np.mean(losses) * 100\n",
    "            results[\"mean\"][name] = np.mean(means) * 100\n",
    "        except Exception as e:\n",
    "            print(f\"[Trial {j}] Error in {name}: {e}\")\n",
    "\n",
    "    # === GMM model ===\n",
    "    try:\n",
    "        scaler_s = StandardScaler()\n",
    "        scaler_xi = StandardScaler()\n",
    "        s_train_std = scaler_s.fit_transform(s_train)\n",
    "        xi_train_std = scaler_xi.fit_transform(xi_train)\n",
    "        data_train_std = np.concatenate([s_train_std, xi_train_std], axis=1)\n",
    "        data_train_tensor = torch.tensor(data_train_std, dtype=torch.float32, device=device)\n",
    "\n",
    "        best_K_GMM = select_K_by_AIC(data_train_std, max_K=max_K)\n",
    "        results[\"K_GMM\"] = best_K_GMM\n",
    "\n",
    "        nfm, _ = train_nf_model(5 + 399, best_K_GMM, hidden_node, hidden_layer, bins, block_size, total_epoch, data_train_tensor, device)\n",
    "\n",
    "        gmm_x = GaussianMixture(n_components=best_K_GMM, covariance_type='diag', reg_covar=1e-2, random_state = base + j).fit(data_train_std)\n",
    "        mu_x, diag_sig_x, p_x = gmm_x.means_, gmm_x.covariances_, gmm_x.weights_\n",
    "        sig_x = np.array([np.diag(diag_sig_x[k]) for k in range(best_K_GMM)])\n",
    "\n",
    "        s_val = s_val.reshape(1, -1)\n",
    "        s_val_std = scaler_s.transform(s_val) \n",
    "        s_vec = s_val_std.ravel()\n",
    "\n",
    "        mu_cond_x, cov_cond_x, p_cond_x = transforming_conditional(s=s_vec, num_components=best_K_GMM, mu_k=mu_x, sig_k=sig_x, p_k=p_x, dim_s=dim_s)\n",
    "        xi_hat_std = (p_cond_x[:, None] * mu_cond_x).sum(axis=0, keepdims=True)\n",
    "\n",
    "        gamma_std = np.hstack([s_val_std, xi_hat_std])                      # (1, dim_s + dim_xi)\n",
    "        gamma_tensor = torch.tensor(gamma_std, dtype=torch.float32, device=device)\n",
    "        z_s = inverse(nfm, gamma_tensor)[:, :dim_s][0]                      # (dim_s,)\n",
    "\n",
    "        z_train = inverse(nfm, data_train_tensor)\n",
    "        gmm_z = GaussianMixture(n_components=best_K_GMM, covariance_type='diag', reg_covar=1e-2, random_state = base + j).fit(z_train)\n",
    "        mu_z, diag_sig_z, p_z = gmm_z.means_, gmm_z.covariances_, gmm_z.weights_\n",
    "        sig_z = np.array([np.diag(diag_sig_z[k]) for k in range(best_K_GMM)])\n",
    "\n",
    "        mu_cond_z, cov_cond_z, p_cond_z = transforming_conditional(s=z_s, num_components=best_K_GMM, mu_k=mu_z, sig_k=sig_z, p_k=p_z, dim_s=dim_s)\n",
    "        z_xi_sample = MC_sampling(best_K_GMM, 1000, mu_cond_z, cov_cond_z, p_cond_z)\n",
    "        z_full = np.hstack([np.repeat(z_s.reshape(1, -1), len(z_xi_sample), axis=0), z_xi_sample])\n",
    "\n",
    "        z_tensor = torch.tensor(z_full, dtype=torch.float32, device=device)\n",
    "        x_gen_std = forward(nfm, z_tensor)\n",
    "        xi_MC = scaler_xi.inverse_transform(x_gen_std[:, dim_s:])  # (1000, dim_xi)                 \n",
    "\n",
    "        x_GMM = Portfolio_2_Wass_MCVaR(xi_MC, best_eps_GMM, tau, eta)\n",
    "        results[\"loss\"][\"GMM\"] = np.mean(oos_loss_portfolio(x_GMM, xi_val_day, tau, eta)) * 100\n",
    "        results[\"mean\"][\"GMM\"] = np.mean(oos_mean_portfolio(x_GMM, xi_val_day)) * 100\n",
    "    except Exception as e:\n",
    "        print(f\"[Trial {j}] Error in GMM: {e}\")\n",
    "\n",
    "    # === Flatten results ===\n",
    "    flattened = {\"Trial\": results[\"Trial\"], \"K_GMM\": results[\"K_GMM\"]}\n",
    "    for metric in metrics:\n",
    "        for method in method_names:\n",
    "            flattened[f\"{metric}_{method}\"] = results[metric][method]\n",
    "    return flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b93bca6f65a992",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-07-24T22:54:29.532006Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t3/2c1kbnd505j_ylmc4_yw3ry00000gq/T/ipykernel_31589/1828125045.py:7: RuntimeWarning: invalid value encountered in divide\n",
      "  return weights / np.sum(weights)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Side info 0: selected bandwidth h = 0.0100\n",
      "Side info 1: selected bandwidth h = 6.9519\n",
      "Side info 2: selected bandwidth h = 3.3598\n",
      "Side info 3: selected bandwidth h = 0.2637\n",
      "Side info 4: selected bandwidth h = 10.0000\n",
      "[DRMC] eps=0.0500, total_loss=-27.5922\n",
      "[DRMC] eps=0.1000, total_loss=-26.0765\n",
      "[DRMC] eps=0.2500, total_loss=-24.7130\n",
      "[CV-DRMC Finished] best_eps_DRMC = 0.05\n",
      "[CMC] quantile=0.05, loss=-149.7281\n",
      "[CMC] quantile=0.10, loss=-177.9288\n",
      "[CMC] quantile=0.25, loss=-106.9434\n",
      "[CV_CMC] best_quantile_CMC = 0.1\n",
      "[DRCMC] gamma=0.05, rho=0.05, loss=-22.1763\n",
      "[DRCMC] gamma=0.05, rho=0.10, loss=-22.2157\n",
      "[DRCMC] gamma=0.05, rho=0.25, loss=-22.2399\n",
      "[DRCMC] gamma=0.10, rho=0.05, loss=-22.2473\n",
      "[DRCMC] gamma=0.10, rho=0.10, loss=-22.2293\n",
      "[DRCMC] gamma=0.10, rho=0.25, loss=-22.2459\n",
      "[DRCMC] gamma=0.25, rho=0.05, loss=-22.2452\n",
      "[DRCMC] gamma=0.25, rho=0.10, loss=-22.2498\n",
      "[DRCMC] gamma=0.25, rho=0.25, loss=-22.2555\n",
      "[CV-DRCMC] best_gamma_q_DRCMC = 0.25, best_eps_DRCMC = 0.25\n",
      "[OTCMC] quantile=0.0500, rho/rho_min=1.05, loss=-22.2541\n",
      "[OTCMC] quantile=0.0500, rho/rho_min=1.10, loss=-22.2514\n",
      "[OTCMC] quantile=0.0500, rho/rho_min=1.15, loss=-22.2484\n",
      "[OTCMC] quantile=0.1000, rho/rho_min=1.05, loss=-22.2574\n",
      "[OTCMC] quantile=0.1000, rho/rho_min=1.10, loss=-22.2530\n",
      "[OTCMC] quantile=0.1000, rho/rho_min=1.15, loss=-22.2506\n",
      "[OTCMC] quantile=0.1500, rho/rho_min=1.05, loss=-22.2529\n",
      "[OTCMC] quantile=0.1500, rho/rho_min=1.10, loss=-22.2495\n",
      "[OTCMC] quantile=0.1500, rho/rho_min=1.15, loss=-22.2477\n",
      "[CV-OTCMC] best_quantile_OTCMC = 0.1, best_eps_OTCMC = 1.05\n",
      "⏹️ Early stopping at epoch 36, best val loss: 499.7039\n",
      "⏹️ Early stopping at epoch 37, best val loss: 449.9680\n",
      "⏹️ Early stopping at epoch 37, best val loss: 476.5477\n",
      "⏹️ Early stopping at epoch 37, best val loss: 447.8051\n",
      "⏹️ Early stopping at epoch 37, best val loss: 449.3923\n",
      "⏹️ Early stopping at epoch 37, best val loss: 471.8872\n",
      "⏹️ Early stopping at epoch 38, best val loss: 338.9618\n",
      "⏹️ Early stopping at epoch 38, best val loss: 454.2087\n",
      "⏹️ Early stopping at epoch 38, best val loss: 447.9323\n",
      "⏹️ Early stopping at epoch 37, best val loss: 460.8387\n",
      "⏹️ Early stopping at epoch 37, best val loss: 461.1840\n",
      "⏹️ Early stopping at epoch 38, best val loss: 407.3602\n",
      "⏹️ Early stopping at epoch 38, best val loss: 466.3293\n",
      "⏹️ Early stopping at epoch 38, best val loss: 470.4157\n",
      "⏹️ Early stopping at epoch 39, best val loss: 345.6844\n",
      "⏹️ Early stopping at epoch 39, best val loss: 307.8245\n",
      "⏹️ Early stopping at epoch 39, best val loss: 336.2834\n",
      "⏹️ Early stopping at epoch 39, best val loss: 350.3026\n",
      "⏹️ Early stopping at epoch 39, best val loss: 341.2159\n",
      "⏹️ Early stopping at epoch 44, best val loss: 446.4001\n",
      "[GMM-CV] j=302 eps=0.0100, loss=62.3709\n",
      "[GMM-CV] j=343 eps=0.0100, loss=1.9783\n",
      "[GMM-CV] j=370 eps=0.0100, loss=-2.7826\n",
      "[GMM-CV] j=378 eps=0.0100, loss=-2.8725\n",
      "[GMM-CV] j=302 eps=0.0500, loss=46.2364\n",
      "[GMM-CV] j=387 eps=0.0100, loss=-1.5364\n",
      "[GMM-CV] j=14 eps=0.0100, loss=-4.7727\n",
      "[GMM-CV] j=10 eps=0.0100, loss=-2.5144\n",
      "[GMM-CV] j=18 eps=0.0100, loss=-0.6073\n",
      "[GMM-CV] j=411 eps=0.0100, loss=1.8662\n",
      "[GMM-CV] j=132 eps=0.0100, loss=-2.5105\n",
      "[GMM-CV] j=189 eps=0.0100, loss=8.4793\n",
      "[GMM-CV] j=39 eps=0.0100, loss=2.2164\n",
      "[GMM-CV] j=43 eps=0.0100, loss=5.2613\n",
      "[GMM-CV] j=412 eps=0.0100, loss=2.4415\n",
      "[GMM-CV] j=123 eps=0.0100, loss=-4.7138\n",
      "[GMM-CV] j=335 eps=0.0100, loss=22.6518\n",
      "[GMM-CV] j=257 eps=0.0100, loss=-4.3264\n",
      "[GMM-CV] j=234 eps=0.0100, loss=-1.3917\n",
      "[GMM-CV] j=235 eps=0.0100, loss=-1.6641\n",
      "[GMM-CV] j=34 eps=0.0100, loss=1.9086\n",
      "[GMM-CV] j=343 eps=0.0500, loss=5.8572\n",
      "[GMM-CV] j=370 eps=0.0500, loss=-2.5170\n",
      "[GMM-CV] j=378 eps=0.0500, loss=-3.3370\n",
      "[GMM-CV] j=302 eps=0.1000, loss=46.5616\n",
      "[GMM-CV] j=335 eps=0.0500, loss=18.8871\n",
      "[GMM-CV] j=387 eps=0.0500, loss=-0.7289\n",
      "[GMM-CV] j=18 eps=0.0500, loss=-1.0726\n",
      "[GMM-CV] j=14 eps=0.0500, loss=-2.0468\n",
      "[GMM-CV] j=10 eps=0.0500, loss=-0.8244\n",
      "[GMM-CV] j=132 eps=0.0500, loss=-2.8220\n",
      "[GMM-CV] j=411 eps=0.0500, loss=3.0050\n",
      "[GMM-CV] j=189 eps=0.0500, loss=9.7240\n",
      "[GMM-CV] j=39 eps=0.0500, loss=1.3297\n",
      "[GMM-CV] j=43 eps=0.0500, loss=4.5469\n",
      "[GMM-CV] j=123 eps=0.0500, loss=-4.6821\n",
      "[GMM-CV] j=257 eps=0.0500, loss=-3.3369\n",
      "[GMM-CV] j=34 eps=0.0500, loss=1.7522\n",
      "[GMM-CV] j=412 eps=0.0500, loss=1.7292\n",
      "[GMM-CV] j=234 eps=0.0500, loss=-1.2320\n",
      "[GMM-CV] j=235 eps=0.0500, loss=-3.8371\n",
      "[GMM-CV] j=343 eps=0.1000, loss=8.8387\n",
      "[GMM-CV] j=370 eps=0.1000, loss=-2.0006\n",
      "[GMM-CV] j=335 eps=0.1000, loss=19.0602\n",
      "[GMM-CV] j=378 eps=0.1000, loss=-3.4713\n",
      "[GMM-CV] j=302 eps=0.5000, loss=61.5942\n",
      "[GMM-CV] j=387 eps=0.1000, loss=-0.2870\n",
      "[GMM-CV] j=18 eps=0.1000, loss=-1.3662\n",
      "[GMM-CV] j=10 eps=0.1000, loss=-1.0658\n",
      "[GMM-CV] j=14 eps=0.1000, loss=-1.0657\n",
      "[GMM-CV] j=411 eps=0.1000, loss=3.0076\n",
      "[GMM-CV] j=132 eps=0.1000, loss=-3.3974\n",
      "[GMM-CV] j=189 eps=0.1000, loss=10.2063\n",
      "[GMM-CV] j=39 eps=0.1000, loss=0.9844\n",
      "[GMM-CV] j=257 eps=0.1000, loss=-3.1245\n",
      "[GMM-CV] j=43 eps=0.1000, loss=4.5287\n",
      "[GMM-CV] j=34 eps=0.1000, loss=1.7559\n",
      "[GMM-CV] j=123 eps=0.1000, loss=-4.7894\n",
      "[GMM-CV] j=234 eps=0.1000, loss=-1.2888\n",
      "[GMM-CV] j=235 eps=0.1000, loss=-4.4811\n",
      "[GMM-CV] j=412 eps=0.1000, loss=2.1285\n",
      "[GMM-CV] j=343 eps=0.5000, loss=14.7875\n",
      "[GMM-CV] j=370 eps=0.5000, loss=-1.3044\n",
      "[GMM-CV] j=335 eps=0.5000, loss=19.8783\n",
      "[GMM-CV] j=378 eps=0.5000, loss=-3.7322\n",
      "[GMM-CV] j=302 eps=1.0000, loss=66.9051\n",
      "[GMM-CV] j=14 eps=0.5000, loss=-0.1654\n",
      "[GMM-CV] j=387 eps=0.5000, loss=0.7042\n",
      "[GMM-CV] j=10 eps=0.5000, loss=-1.6204\n",
      "[GMM-CV] j=411 eps=0.5000, loss=3.0673\n",
      "[GMM-CV] j=18 eps=0.5000, loss=-1.1812\n",
      "[GMM-CV] j=189 eps=0.5000, loss=10.7480\n",
      "[GMM-CV] j=132 eps=0.5000, loss=-4.0562\n",
      "[GMM-CV] j=257 eps=0.5000, loss=-2.9290\n",
      "[GMM-CV] j=39 eps=0.5000, loss=0.7607\n",
      "[GMM-CV] j=123 eps=0.5000, loss=-4.9750\n",
      "[GMM-CV] j=34 eps=0.5000, loss=1.7271\n",
      "[GMM-CV] j=43 eps=0.5000, loss=4.6613\n",
      "[GMM-CV] j=234 eps=0.5000, loss=-1.3218\n",
      "[GMM-CV] j=412 eps=0.5000, loss=2.7850\n",
      "[GMM-CV] j=235 eps=0.5000, loss=-5.2408\n",
      "⏹️ Early stopping at epoch 37, best val loss: 437.8546\n",
      "[GMM-CV] j=343 eps=1.0000, loss=15.6406\n",
      "[GMM-CV] j=335 eps=1.0000, loss=20.0374\n",
      "[GMM-CV] j=370 eps=1.0000, loss=-1.2100\n",
      "[GMM-CV] j=378 eps=1.0000, loss=-3.7649\n",
      "⏹️ Early stopping at epoch 39, best val loss: 340.0328\n",
      "⏹️ Early stopping at epoch 37, best val loss: 460.0766\n",
      "[GMM-CV] j=387 eps=1.0000, loss=0.8764\n",
      "⏹️ Early stopping at epoch 41, best val loss: 313.5582\n",
      "[GMM-CV] j=14 eps=1.0000, loss=-0.0623\n",
      "[GMM-CV] j=18 eps=1.0000, loss=-1.1538\n",
      "[GMM-CV] j=10 eps=1.0000, loss=-1.6831\n",
      "[GMM-CV] j=411 eps=1.0000, loss=3.0680\n",
      "[GMM-CV] j=257 eps=1.0000, loss=-2.9070\n",
      "[GMM-CV] j=39 eps=1.0000, loss=0.7347\n",
      "[GMM-CV] j=189 eps=1.0000, loss=10.8189\n",
      "[GMM-CV] j=34 eps=1.0000, loss=1.7238\n",
      "[GMM-CV] j=43 eps=1.0000, loss=4.6798\n",
      "[GMM-CV] j=132 eps=1.0000, loss=-4.1427\n",
      "[GMM-CV] j=123 eps=1.0000, loss=-5.0009\n",
      "[GMM-CV] j=234 eps=1.0000, loss=-1.3265\n",
      "[GMM-CV] j=412 eps=1.0000, loss=2.8807\n",
      "[GMM-CV] j=235 eps=1.0000, loss=-5.3409\n",
      "⏹️ Early stopping at epoch 38, best val loss: 336.8371\n",
      "⏹️ Early stopping at epoch 37, best val loss: 459.2666\n",
      "⏹️ Early stopping at epoch 36, best val loss: 476.4268\n",
      "⏹️ Early stopping at epoch 37, best val loss: 466.3651\n",
      "⏹️ Early stopping at epoch 37, best val loss: 472.0558\n",
      "⏹️ Early stopping at epoch 36, best val loss: 451.9940\n",
      "⏹️ Early stopping at epoch 38, best val loss: 488.3961\n",
      "⏹️ Early stopping at epoch 36, best val loss: 492.9805\n",
      "⏹️ Early stopping at epoch 37, best val loss: 474.3305\n",
      "⏹️ Early stopping at epoch 37, best val loss: 477.3818\n",
      "⏹️ Early stopping at epoch 39, best val loss: 371.7340\n",
      "⏹️ Early stopping at epoch 38, best val loss: 343.5177\n",
      "⏹️ Early stopping at epoch 40, best val loss: 345.6098\n",
      "⏹️ Early stopping at epoch 40, best val loss: 339.9880\n",
      "⏹️ Early stopping at epoch 39, best val loss: 441.3178\n",
      "⏹️ Early stopping at epoch 40, best val loss: 340.9968\n",
      "[GMM-CV] j=7 eps=0.0100, loss=0.5931\n",
      "[GMM-CV] j=310 eps=0.0100, loss=-17.0265\n",
      "[GMM-CV] j=389 eps=0.0100, loss=-5.1265\n",
      "[GMM-CV] j=291 eps=0.0100, loss=13.8824\n",
      "[GMM-CV] j=490 eps=0.0100, loss=0.2285\n",
      "[GMM-CV] j=2 eps=0.0100, loss=-4.5607\n",
      "[GMM-CV] j=7 eps=0.0500, loss=-0.2264\n",
      "[GMM-CV] j=325 eps=0.0100, loss=-7.6213\n",
      "[GMM-CV] j=62 eps=0.0100, loss=0.1172\n",
      "[GMM-CV] j=81 eps=0.0100, loss=-1.0064\n",
      "[GMM-CV] j=129 eps=0.0100, loss=-1.8209\n",
      "[GMM-CV] j=444 eps=0.0100, loss=4.4201\n",
      "[GMM-CV] j=284 eps=0.0100, loss=-5.5341\n",
      "[GMM-CV] j=1 eps=0.0100, loss=-0.6609\n",
      "[GMM-CV] j=426 eps=0.0100, loss=11.1442\n",
      "[GMM-CV] j=310 eps=0.0500, loss=-27.3478\n",
      "[GMM-CV] j=16 eps=0.0100, loss=2.0248\n",
      "[GMM-CV] j=141 eps=0.0100, loss=4.4035\n",
      "[GMM-CV] j=85 eps=0.0100, loss=1.8912\n",
      "[GMM-CV] j=200 eps=0.0100, loss=-3.7705\n",
      "[GMM-CV] j=456 eps=0.0100, loss=-5.1540\n",
      "[GMM-CV] j=389 eps=0.0500, loss=-1.1872\n",
      "[GMM-CV] j=422 eps=0.0100, loss=24.5067\n",
      "[GMM-CV] j=490 eps=0.0500, loss=1.7087\n",
      "[GMM-CV] j=291 eps=0.0500, loss=15.9469\n",
      "[GMM-CV] j=2 eps=0.0500, loss=-8.1805\n",
      "[GMM-CV] j=7 eps=0.1000, loss=-0.4979\n",
      "[GMM-CV] j=325 eps=0.0500, loss=-18.0765\n",
      "[GMM-CV] j=62 eps=0.0500, loss=0.5234\n",
      "[GMM-CV] j=310 eps=0.1000, loss=-30.5803\n",
      "[GMM-CV] j=81 eps=0.0500, loss=-1.9697\n",
      "[GMM-CV] j=129 eps=0.0500, loss=-0.6057\n",
      "[GMM-CV] j=284 eps=0.0500, loss=-2.0760\n",
      "[GMM-CV] j=444 eps=0.0500, loss=4.6460\n",
      "[GMM-CV] j=1 eps=0.0500, loss=2.4202\n",
      "[GMM-CV] j=426 eps=0.0500, loss=8.5564\n",
      "[GMM-CV] j=16 eps=0.0500, loss=-2.5059\n",
      "[GMM-CV] j=141 eps=0.0500, loss=3.3794\n",
      "[GMM-CV] j=85 eps=0.0500, loss=2.2272\n",
      "[GMM-CV] j=389 eps=0.1000, loss=0.4695\n",
      "[GMM-CV] j=200 eps=0.0500, loss=-2.6324\n",
      "[GMM-CV] j=456 eps=0.0500, loss=-3.4349\n",
      "[GMM-CV] j=490 eps=0.1000, loss=1.5167\n",
      "[GMM-CV] j=422 eps=0.0500, loss=20.7774\n",
      "[GMM-CV] j=291 eps=0.1000, loss=15.1174\n",
      "[GMM-CV] j=2 eps=0.1000, loss=-12.5739\n",
      "[GMM-CV] j=7 eps=0.5000, loss=-0.7701\n",
      "[GMM-CV] j=310 eps=0.5000, loss=-37.4244\n",
      "[GMM-CV] j=325 eps=0.1000, loss=-20.6003\n",
      "[GMM-CV] j=62 eps=0.1000, loss=0.4399\n",
      "[GMM-CV] j=81 eps=0.1000, loss=-2.2232\n",
      "[GMM-CV] j=129 eps=0.1000, loss=-0.2506\n",
      "[GMM-CV] j=284 eps=0.1000, loss=-1.8980\n",
      "[GMM-CV] j=444 eps=0.1000, loss=4.9193\n",
      "[GMM-CV] j=426 eps=0.1000, loss=8.4809\n",
      "[GMM-CV] j=1 eps=0.1000, loss=6.7852\n",
      "[GMM-CV] j=16 eps=0.1000, loss=-4.0050\n",
      "[GMM-CV] j=389 eps=0.5000, loss=2.6359\n",
      "[GMM-CV] j=141 eps=0.1000, loss=3.1772\n",
      "[GMM-CV] j=85 eps=0.1000, loss=2.4207\n",
      "[GMM-CV] j=200 eps=0.1000, loss=-2.5130\n",
      "[GMM-CV] j=490 eps=0.5000, loss=1.1180\n",
      "[GMM-CV] j=456 eps=0.1000, loss=-4.8514\n",
      "[GMM-CV] j=422 eps=0.1000, loss=18.8146\n",
      "[GMM-CV] j=291 eps=0.5000, loss=9.8461\n",
      "[GMM-CV] j=2 eps=0.5000, loss=-18.0931\n",
      "[GMM-CV] j=7 eps=1.0000, loss=-0.8069\n",
      "[GMM-CV] j=310 eps=1.0000, loss=-37.6222\n",
      "[GMM-CV] j=389 eps=1.0000, loss=2.9272\n",
      "[GMM-CV] j=325 eps=0.5000, loss=-24.6331\n",
      "[GMM-CV] j=62 eps=0.5000, loss=0.3468\n",
      "[GMM-CV] j=81 eps=0.5000, loss=-2.5222\n",
      "[GMM-CV] j=284 eps=0.5000, loss=-1.8628\n",
      "[GMM-CV] j=129 eps=0.5000, loss=0.0064\n",
      "[GMM-CV] j=444 eps=0.5000, loss=5.6351\n",
      "[GMM-CV] j=426 eps=0.5000, loss=8.8214\n",
      "[GMM-CV] j=16 eps=0.5000, loss=-5.3070\n",
      "[GMM-CV] j=1 eps=0.5000, loss=10.8339\n",
      "[GMM-CV] j=141 eps=0.5000, loss=3.3719\n",
      "[GMM-CV] j=85 eps=0.5000, loss=2.6527\n",
      "[GMM-CV] j=490 eps=1.0000, loss=1.0607\n",
      "⏹️ Early stopping at epoch 36, best val loss: 476.9341\n",
      "[GMM-CV] j=200 eps=0.5000, loss=-2.4109\n",
      "[GMM-CV] j=456 eps=0.5000, loss=-7.0317\n",
      "⏹️ Early stopping at epoch 39, best val loss: 462.7432\n",
      "[GMM-CV] j=422 eps=0.5000, loss=15.3562\n",
      "⏹️ Early stopping at epoch 40, best val loss: 380.8202\n",
      "[GMM-CV] j=291 eps=1.0000, loss=8.7869\n",
      "⏹️ Early stopping at epoch 37, best val loss: 441.4693\n",
      "[GMM-CV] j=2 eps=1.0000, loss=-18.7411\n",
      "⏹️ Early stopping at epoch 37, best val loss: 483.9146\n",
      "[GMM-CV] j=62 eps=1.0000, loss=0.3344\n",
      "[GMM-CV] j=325 eps=1.0000, loss=-25.1775\n",
      "[GMM-CV] j=426 eps=1.0000, loss=8.9193\n",
      "[GMM-CV] j=16 eps=1.0000, loss=-5.4707\n",
      "[GMM-CV] j=129 eps=1.0000, loss=0.0405\n",
      "[GMM-CV] j=81 eps=1.0000, loss=-2.5608\n",
      "[GMM-CV] j=444 eps=1.0000, loss=5.7588\n",
      "[GMM-CV] j=284 eps=1.0000, loss=-1.8656\n",
      "[GMM-CV] j=1 eps=1.0000, loss=11.2752\n",
      "[GMM-CV] j=141 eps=1.0000, loss=3.3993\n",
      "[GMM-CV] j=85 eps=1.0000, loss=2.6808\n",
      "[GMM-CV] j=200 eps=1.0000, loss=-2.3988\n",
      "⏹️ Early stopping at epoch 37, best val loss: 477.0674\n",
      "[GMM-CV] j=456 eps=1.0000, loss=-7.3521\n",
      "[GMM-CV] j=422 eps=1.0000, loss=14.6226\n",
      "⏹️ Early stopping at epoch 37, best val loss: 466.4203\n",
      "⏹️ Early stopping at epoch 37, best val loss: 456.3369\n",
      "⏹️ Early stopping at epoch 37, best val loss: 472.0647\n",
      "⏹️ Early stopping at epoch 41, best val loss: 309.5322\n",
      "[GMM-CV] j=353 eps=0.0100, loss=5.1589\n",
      "[GMM-CV] j=267 eps=0.0100, loss=4.8846\n",
      "[GMM-CV] j=298 eps=0.0100, loss=-12.8881\n",
      "[GMM-CV] j=209 eps=0.0100, loss=-4.9213\n",
      "[GMM-CV] j=4 eps=0.0100, loss=-7.3234\n",
      "[GMM-CV] j=353 eps=0.0500, loss=3.4610\n",
      "[GMM-CV] j=267 eps=0.0500, loss=5.0295\n",
      "[GMM-CV] j=298 eps=0.0500, loss=-13.7229\n",
      "[GMM-CV] j=320 eps=0.0100, loss=-5.6186\n",
      "[GMM-CV] j=209 eps=0.0500, loss=-2.0578\n",
      "[GMM-CV] j=147 eps=0.0100, loss=5.6739\n",
      "[GMM-CV] j=4 eps=0.0500, loss=-6.6160\n",
      "[GMM-CV] j=353 eps=0.1000, loss=3.2712\n",
      "[GMM-CV] j=263 eps=0.0100, loss=-2.3072\n",
      "[GMM-CV] j=237 eps=0.0100, loss=-0.3187\n",
      "[GMM-CV] j=265 eps=0.0100, loss=-1.3719\n",
      "[GMM-CV] j=267 eps=0.1000, loss=5.4142\n",
      "[GMM-CV] j=298 eps=0.1000, loss=-17.4944\n",
      "[GMM-CV] j=320 eps=0.0500, loss=-7.1651\n",
      "[GMM-CV] j=209 eps=0.1000, loss=-1.5007\n",
      "[GMM-CV] j=147 eps=0.0500, loss=4.2059\n",
      "[GMM-CV] j=353 eps=0.5000, loss=3.8808\n",
      "[GMM-CV] j=4 eps=0.1000, loss=-6.7052\n",
      "[GMM-CV] j=263 eps=0.0500, loss=-2.1136\n",
      "[GMM-CV] j=237 eps=0.0500, loss=0.5512\n",
      "[GMM-CV] j=265 eps=0.0500, loss=-0.0404\n",
      "[GMM-CV] j=267 eps=0.5000, loss=5.8551\n",
      "[GMM-CV] j=320 eps=0.1000, loss=-8.7955\n",
      "[GMM-CV] j=298 eps=0.5000, loss=-26.4232\n",
      "[GMM-CV] j=209 eps=0.5000, loss=-1.0108\n",
      "[GMM-CV] j=147 eps=0.1000, loss=4.1145\n",
      "[GMM-CV] j=353 eps=1.0000, loss=3.9853\n",
      "[GMM-CV] j=4 eps=0.5000, loss=-6.4024\n",
      "[GMM-CV] j=263 eps=0.1000, loss=-1.9062\n",
      "[GMM-CV] j=265 eps=0.1000, loss=0.2110\n",
      "[GMM-CV] j=237 eps=0.1000, loss=0.7508\n",
      "[GMM-CV] j=267 eps=1.0000, loss=5.9163\n",
      "[GMM-CV] j=320 eps=0.5000, loss=-13.0338\n",
      "[GMM-CV] j=298 eps=1.0000, loss=-28.3087\n",
      "[GMM-CV] j=209 eps=1.0000, loss=-0.9507\n",
      "[GMM-CV] j=147 eps=0.5000, loss=4.1115\n",
      "[GMM-CV] j=4 eps=1.0000, loss=-6.3433\n",
      "[GMM-CV] j=263 eps=0.5000, loss=-1.6624\n",
      "[GMM-CV] j=265 eps=0.5000, loss=0.4608\n",
      "[GMM-CV] j=237 eps=0.5000, loss=0.8838\n",
      "[GMM-CV] j=320 eps=1.0000, loss=-14.2549\n",
      "[GMM-CV] j=147 eps=1.0000, loss=4.1106\n",
      "[GMM-CV] j=263 eps=1.0000, loss=-1.6304\n",
      "[GMM-CV] j=265 eps=1.0000, loss=0.4941\n",
      "[GMM-CV] j=237 eps=1.0000, loss=0.8981\n",
      "[GMM-CV] eps=0.0100, total_loss=71.3798\n",
      "[GMM-CV] eps=0.0500, total_loss=40.1051\n",
      "[GMM-CV] eps=0.1000, total_loss=30.2414\n",
      "[GMM-CV] eps=0.5000, total_loss=21.4159\n",
      "[GMM-CV] eps=1.0000, total_loss=22.5001\n",
      "[GMM-CV Finished] K=2.72, eps=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 20/252 [00:20<00:01, 137.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏹️ Early stopping at epoch 39, best val loss: 422.6474\n",
      "⏹️ Early stopping at epoch 38, best val loss: 383.4891\n",
      "⏹️ Early stopping at epoch 38, best val loss: 354.0795\n",
      "⏹️ Early stopping at epoch 39, best val loss: 333.5549\n",
      "⏹️ Early stopping at epoch 42, best val loss: 338.9845\n",
      "⏹️ Early stopping at epoch 40, best val loss: 329.4129\n",
      "⏹️ Early stopping at epoch 40, best val loss: 326.9452\n",
      "⏹️ Early stopping at epoch 39, best val loss: 351.8180\n",
      "⏹️ Early stopping at epoch 39, best val loss: 340.0187\n",
      "⏹️ Early stopping at epoch 40, best val loss: 332.6748\n",
      "⏹️ Early stopping at epoch 40, best val loss: 361.4333\n",
      "⏹️ Early stopping at epoch 42, best val loss: 327.4332\n",
      "⏹️ Early stopping at epoch 40, best val loss: 335.7787\n",
      "⏹️ Early stopping at epoch 41, best val loss: 360.4428\n",
      "⏹️ Early stopping at epoch 41, best val loss: 340.5678\n",
      "⏹️ Early stopping at epoch 41, best val loss: 334.0247\n",
      "⏹️ Early stopping at epoch 42, best val loss: 314.2527\n",
      "⏹️ Early stopping at epoch 42, best val loss: 336.4263\n",
      "⏹️ Early stopping at epoch 43, best val loss: 344.2090\n",
      "⏹️ Early stopping at epoch 42, best val loss: 355.5120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 40/252 [04:19<26:57,  7.63s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏹️ Early stopping at epoch 40, best val loss: 387.8802\n",
      "⏹️ Early stopping at epoch 40, best val loss: 368.6300\n",
      "⏹️ Early stopping at epoch 40, best val loss: 371.1656\n",
      "⏹️ Early stopping at epoch 38, best val loss: 387.0206\n",
      "⏹️ Early stopping at epoch 40, best val loss: 339.2568\n",
      "⏹️ Early stopping at epoch 38, best val loss: 356.3312\n",
      "⏹️ Early stopping at epoch 38, best val loss: 396.7624\n",
      "⏹️ Early stopping at epoch 38, best val loss: 356.5568\n",
      "⏹️ Early stopping at epoch 38, best val loss: 342.5642\n",
      "⏹️ Early stopping at epoch 39, best val loss: 329.9566\n",
      "⏹️ Early stopping at epoch 41, best val loss: 339.3880\n",
      "⏹️ Early stopping at epoch 38, best val loss: 409.3329\n",
      "⏹️ Early stopping at epoch 41, best val loss: 324.4147\n",
      "⏹️ Early stopping at epoch 38, best val loss: 345.3974\n",
      "⏹️ Early stopping at epoch 40, best val loss: 353.1592\n",
      "⏹️ Early stopping at epoch 41, best val loss: 361.1432\n",
      "⏹️ Early stopping at epoch 41, best val loss: 322.9090\n",
      "⏹️ Early stopping at epoch 42, best val loss: 338.3307\n",
      "⏹️ Early stopping at epoch 39, best val loss: 380.2280\n",
      "⏹️ Early stopping at epoch 40, best val loss: 355.1030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 60/252 [08:54<33:19, 10.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏹️ Early stopping at epoch 42, best val loss: 341.5740\n",
      "⏹️ Early stopping at epoch 41, best val loss: 341.7833\n",
      "⏹️ Early stopping at epoch 41, best val loss: 345.2389\n",
      "⏹️ Early stopping at epoch 42, best val loss: 315.4269\n",
      "⏹️ Early stopping at epoch 40, best val loss: 340.0246\n",
      "⏹️ Early stopping at epoch 38, best val loss: 420.0184\n",
      "⏹️ Early stopping at epoch 41, best val loss: 336.0327\n",
      "⏹️ Early stopping at epoch 39, best val loss: 316.4702\n",
      "⏹️ Early stopping at epoch 40, best val loss: 346.8635\n",
      "⏹️ Early stopping at epoch 42, best val loss: 336.7057\n",
      "⏹️ Early stopping at epoch 41, best val loss: 344.0802\n",
      "⏹️ Early stopping at epoch 39, best val loss: 357.5720\n",
      "⏹️ Early stopping at epoch 38, best val loss: 365.9744\n",
      "⏹️ Early stopping at epoch 41, best val loss: 368.2577\n",
      "⏹️ Early stopping at epoch 39, best val loss: 340.1085\n",
      "⏹️ Early stopping at epoch 40, best val loss: 418.9615\n",
      "⏹️ Early stopping at epoch 39, best val loss: 391.0729\n",
      "⏹️ Early stopping at epoch 43, best val loss: 320.5492\n",
      "⏹️ Early stopping at epoch 40, best val loss: 345.8032\n",
      "⏹️ Early stopping at epoch 40, best val loss: 363.9028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 80/252 [14:02<35:31, 12.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏹️ Early stopping at epoch 40, best val loss: 332.8678\n",
      "⏹️ Early stopping at epoch 40, best val loss: 369.6581\n",
      "⏹️ Early stopping at epoch 40, best val loss: 343.5558\n",
      "⏹️ Early stopping at epoch 39, best val loss: 340.3277\n",
      "⏹️ Early stopping at epoch 41, best val loss: 377.2713\n",
      "⏹️ Early stopping at epoch 39, best val loss: 339.2267\n",
      "⏹️ Early stopping at epoch 40, best val loss: 359.8116\n",
      "⏹️ Early stopping at epoch 41, best val loss: 345.1405\n",
      "⏹️ Early stopping at epoch 39, best val loss: 365.5367\n",
      "⏹️ Early stopping at epoch 42, best val loss: 349.1380\n",
      "⏹️ Early stopping at epoch 42, best val loss: 327.9343\n",
      "⏹️ Early stopping at epoch 39, best val loss: 331.1034\n",
      "⏹️ Early stopping at epoch 40, best val loss: 356.1425\n",
      "⏹️ Early stopping at epoch 42, best val loss: 359.1866\n",
      "⏹️ Early stopping at epoch 40, best val loss: 320.8601\n",
      "⏹️ Early stopping at epoch 41, best val loss: 344.2770\n",
      "⏹️ Early stopping at epoch 38, best val loss: 333.8329\n",
      "⏹️ Early stopping at epoch 39, best val loss: 393.7184\n",
      "⏹️ Early stopping at epoch 39, best val loss: 356.6481\n",
      "⏹️ Early stopping at epoch 40, best val loss: 336.1010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 100/252 [19:27<34:53, 13.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏹️ Early stopping at epoch 38, best val loss: 389.0210\n",
      "⏹️ Early stopping at epoch 39, best val loss: 338.4200\n",
      "⏹️ Early stopping at epoch 39, best val loss: 329.0978\n",
      "⏹️ Early stopping at epoch 43, best val loss: 360.4283\n",
      "⏹️ Early stopping at epoch 39, best val loss: 355.8497\n",
      "⏹️ Early stopping at epoch 38, best val loss: 404.6847\n",
      "⏹️ Early stopping at epoch 39, best val loss: 391.2231\n",
      "⏹️ Early stopping at epoch 40, best val loss: 369.3433\n",
      "⏹️ Early stopping at epoch 39, best val loss: 371.7058\n",
      "⏹️ Early stopping at epoch 39, best val loss: 364.6295\n",
      "⏹️ Early stopping at epoch 39, best val loss: 356.0192\n",
      "⏹️ Early stopping at epoch 42, best val loss: 340.4077\n",
      "⏹️ Early stopping at epoch 39, best val loss: 403.8477\n",
      "⏹️ Early stopping at epoch 39, best val loss: 367.5837\n",
      "⏹️ Early stopping at epoch 39, best val loss: 354.7426\n",
      "⏹️ Early stopping at epoch 39, best val loss: 355.9026\n",
      "⏹️ Early stopping at epoch 40, best val loss: 353.8963\n",
      "⏹️ Early stopping at epoch 42, best val loss: 356.1426\n",
      "⏹️ Early stopping at epoch 41, best val loss: 362.1420\n",
      "⏹️ Early stopping at epoch 41, best val loss: 370.2069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 120/252 [25:00<32:27, 14.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏹️ Early stopping at epoch 39, best val loss: 377.8043\n",
      "⏹️ Early stopping at epoch 38, best val loss: 379.3455\n",
      "⏹️ Early stopping at epoch 39, best val loss: 355.9066\n",
      "⏹️ Early stopping at epoch 38, best val loss: 387.0553\n",
      "⏹️ Early stopping at epoch 43, best val loss: 331.4168\n",
      "⏹️ Early stopping at epoch 40, best val loss: 353.2634\n",
      "⏹️ Early stopping at epoch 41, best val loss: 351.0064\n",
      "⏹️ Early stopping at epoch 42, best val loss: 381.0307\n",
      "⏹️ Early stopping at epoch 40, best val loss: 345.5097\n",
      "⏹️ Early stopping at epoch 39, best val loss: 330.5447\n",
      "⏹️ Early stopping at epoch 42, best val loss: 349.1621\n",
      "⏹️ Early stopping at epoch 38, best val loss: 343.4776\n",
      "⏹️ Early stopping at epoch 42, best val loss: 339.5114\n",
      "⏹️ Early stopping at epoch 39, best val loss: 355.3033\n",
      "⏹️ Early stopping at epoch 42, best val loss: 377.9112\n",
      "⏹️ Early stopping at epoch 41, best val loss: 353.0242\n",
      "⏹️ Early stopping at epoch 39, best val loss: 341.1018\n",
      "⏹️ Early stopping at epoch 42, best val loss: 369.9012\n",
      "⏹️ Early stopping at epoch 39, best val loss: 339.7597\n",
      "⏹️ Early stopping at epoch 41, best val loss: 367.6311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 140/252 [30:21<28:20, 15.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏹️ Early stopping at epoch 40, best val loss: 357.7880\n",
      "⏹️ Early stopping at epoch 38, best val loss: 357.6513\n",
      "⏹️ Early stopping at epoch 43, best val loss: 323.9693\n",
      "⏹️ Early stopping at epoch 41, best val loss: 380.1758\n",
      "⏹️ Early stopping at epoch 42, best val loss: 318.3873\n",
      "⏹️ Early stopping at epoch 40, best val loss: 354.7029\n",
      "⏹️ Early stopping at epoch 43, best val loss: 342.7015\n",
      "⏹️ Early stopping at epoch 40, best val loss: 354.8472\n",
      "⏹️ Early stopping at epoch 42, best val loss: 364.1978\n",
      "⏹️ Early stopping at epoch 40, best val loss: 383.1084\n",
      "⏹️ Early stopping at epoch 39, best val loss: 361.2729\n",
      "⏹️ Early stopping at epoch 42, best val loss: 321.2832\n",
      "⏹️ Early stopping at epoch 43, best val loss: 369.2578\n",
      "⏹️ Early stopping at epoch 41, best val loss: 328.2502\n",
      "⏹️ Early stopping at epoch 42, best val loss: 358.3524\n",
      "⏹️ Early stopping at epoch 40, best val loss: 353.2980\n",
      "⏹️ Early stopping at epoch 42, best val loss: 334.7814\n",
      "⏹️ Early stopping at epoch 40, best val loss: 327.3541\n",
      "⏹️ Early stopping at epoch 41, best val loss: 333.9252\n",
      "⏹️ Early stopping at epoch 42, best val loss: 357.6360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 160/252 [35:48<23:50, 15.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏹️ Early stopping at epoch 40, best val loss: 353.7577\n",
      "⏹️ Early stopping at epoch 40, best val loss: 355.0843\n",
      "⏹️ Early stopping at epoch 39, best val loss: 368.5713\n",
      "⏹️ Early stopping at epoch 42, best val loss: 360.8847\n",
      "⏹️ Early stopping at epoch 39, best val loss: 362.1388\n",
      "⏹️ Early stopping at epoch 41, best val loss: 365.5618\n",
      "⏹️ Early stopping at epoch 40, best val loss: 380.7002\n",
      "⏹️ Early stopping at epoch 42, best val loss: 349.0020\n",
      "⏹️ Early stopping at epoch 40, best val loss: 349.4785\n",
      "⏹️ Early stopping at epoch 40, best val loss: 348.3909\n",
      "⏹️ Early stopping at epoch 41, best val loss: 337.8170\n",
      "⏹️ Early stopping at epoch 42, best val loss: 345.5187\n",
      "⏹️ Early stopping at epoch 40, best val loss: 342.8629\n",
      "⏹️ Early stopping at epoch 40, best val loss: 359.8344\n",
      "⏹️ Early stopping at epoch 40, best val loss: 370.7479\n",
      "⏹️ Early stopping at epoch 41, best val loss: 331.3852\n",
      "⏹️ Early stopping at epoch 43, best val loss: 343.5678\n",
      "⏹️ Early stopping at epoch 39, best val loss: 386.3278\n",
      "⏹️ Early stopping at epoch 43, best val loss: 316.4005\n",
      "⏹️ Early stopping at epoch 42, best val loss: 366.4751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 180/252 [41:22<19:05, 15.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏹️ Early stopping at epoch 40, best val loss: 352.5240\n",
      "⏹️ Early stopping at epoch 42, best val loss: 393.7143\n",
      "⏹️ Early stopping at epoch 43, best val loss: 316.8986\n",
      "⏹️ Early stopping at epoch 40, best val loss: 374.9836\n",
      "⏹️ Early stopping at epoch 41, best val loss: 359.2109\n",
      "⏹️ Early stopping at epoch 40, best val loss: 338.8047\n",
      "⏹️ Early stopping at epoch 41, best val loss: 387.1754\n",
      "⏹️ Early stopping at epoch 40, best val loss: 327.7187\n",
      "⏹️ Early stopping at epoch 39, best val loss: 342.9884\n",
      "⏹️ Early stopping at epoch 41, best val loss: 344.7219\n",
      "⏹️ Early stopping at epoch 40, best val loss: 362.9520\n",
      "⏹️ Early stopping at epoch 43, best val loss: 331.9252\n",
      "⏹️ Early stopping at epoch 40, best val loss: 349.8240\n",
      "⏹️ Early stopping at epoch 40, best val loss: 386.1262\n",
      "⏹️ Early stopping at epoch 41, best val loss: 348.6972\n",
      "⏹️ Early stopping at epoch 39, best val loss: 350.2803\n",
      "⏹️ Early stopping at epoch 39, best val loss: 363.2504\n",
      "⏹️ Early stopping at epoch 43, best val loss: 343.5835\n",
      "⏹️ Early stopping at epoch 40, best val loss: 366.5392\n",
      "⏹️ Early stopping at epoch 39, best val loss: 378.4660\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 200/252 [46:52<13:57, 16.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏹️ Early stopping at epoch 43, best val loss: 362.1911\n",
      "⏹️ Early stopping at epoch 40, best val loss: 373.5498\n",
      "⏹️ Early stopping at epoch 39, best val loss: 333.2215\n",
      "⏹️ Early stopping at epoch 39, best val loss: 362.7358\n",
      "⏹️ Early stopping at epoch 39, best val loss: 321.7986\n",
      "⏹️ Early stopping at epoch 41, best val loss: 376.4541\n",
      "⏹️ Early stopping at epoch 39, best val loss: 373.5902\n",
      "⏹️ Early stopping at epoch 40, best val loss: 366.9876\n",
      "⏹️ Early stopping at epoch 42, best val loss: 327.3172\n",
      "⏹️ Early stopping at epoch 41, best val loss: 341.9622\n",
      "⏹️ Early stopping at epoch 40, best val loss: 356.5591\n",
      "⏹️ Early stopping at epoch 41, best val loss: 354.7091\n",
      "⏹️ Early stopping at epoch 44, best val loss: 328.8944\n",
      "⏹️ Early stopping at epoch 39, best val loss: 381.5731\n",
      "⏹️ Early stopping at epoch 39, best val loss: 356.8148\n",
      "⏹️ Early stopping at epoch 43, best val loss: 341.9196\n",
      "⏹️ Early stopping at epoch 42, best val loss: 307.8334\n",
      "⏹️ Early stopping at epoch 39, best val loss: 356.8913\n",
      "⏹️ Early stopping at epoch 40, best val loss: 366.9591\n",
      "⏹️ Early stopping at epoch 40, best val loss: 349.3905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 220/252 [52:34<08:44, 16.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏹️ Early stopping at epoch 40, best val loss: 364.2865\n",
      "⏹️ Early stopping at epoch 40, best val loss: 368.6475\n",
      "⏹️ Early stopping at epoch 44, best val loss: 333.8141\n",
      "⏹️ Early stopping at epoch 43, best val loss: 320.4958\n",
      "⏹️ Early stopping at epoch 40, best val loss: 369.5391\n",
      "⏹️ Early stopping at epoch 39, best val loss: 335.5234\n",
      "⏹️ Early stopping at epoch 41, best val loss: 378.8310\n",
      "⏹️ Early stopping at epoch 42, best val loss: 352.8069\n",
      "⏹️ Early stopping at epoch 39, best val loss: 368.9493\n",
      "⏹️ Early stopping at epoch 40, best val loss: 334.8273\n",
      "⏹️ Early stopping at epoch 41, best val loss: 350.7735\n",
      "⏹️ Early stopping at epoch 40, best val loss: 396.5399\n",
      "⏹️ Early stopping at epoch 42, best val loss: 316.2936\n",
      "⏹️ Early stopping at epoch 41, best val loss: 317.5910\n",
      "⏹️ Early stopping at epoch 42, best val loss: 347.5466\n",
      "⏹️ Early stopping at epoch 47, best val loss: 494.0367\n",
      "⏹️ Early stopping at epoch 40, best val loss: 384.9578\n",
      "⏹️ Early stopping at epoch 38, best val loss: 377.6151\n",
      "⏹️ Early stopping at epoch 40, best val loss: 350.9454\n",
      "⏹️ Early stopping at epoch 45, best val loss: 332.8843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 252/252 [58:25<00:00, 13.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏹️ Early stopping at epoch 41, best val loss: 335.8478\n",
      "⏹️ Early stopping at epoch 38, best val loss: 403.4236\n",
      "⏹️ Early stopping at epoch 39, best val loss: 336.8943\n",
      "⏹️ Early stopping at epoch 41, best val loss: 356.3513\n",
      "⏹️ Early stopping at epoch 40, best val loss: 373.9826\n",
      "⏹️ Early stopping at epoch 40, best val loss: 400.9876\n",
      "⏹️ Early stopping at epoch 39, best val loss: 398.7698\n",
      "⏹️ Early stopping at epoch 38, best val loss: 367.8878\n",
      "⏹️ Early stopping at epoch 42, best val loss: 367.1689\n",
      "⏹️ Early stopping at epoch 40, best val loss: 368.4296\n",
      "⏹️ Early stopping at epoch 40, best val loss: 398.9670\n",
      "⏹️ Early stopping at epoch 40, best val loss: 361.1542\n",
      "⏹️ Early stopping at epoch 41, best val loss: 370.9644\n",
      "⏹️ Early stopping at epoch 39, best val loss: 406.7003\n",
      "⏹️ Early stopping at epoch 39, best val loss: 389.2323\n",
      "⏹️ Early stopping at epoch 40, best val loss: 346.6355\n",
      "⏹️ Early stopping at epoch 40, best val loss: 404.3913\n",
      "⏹️ Early stopping at epoch 40, best val loss: 357.9782\n",
      "⏹️ Early stopping at epoch 42, best val loss: 380.4220\n",
      "⏹️ Early stopping at epoch 42, best val loss: 333.4178\n",
      "⏹️ Early stopping at epoch 44, best val loss: 362.2273\n",
      "⏹️ Early stopping at epoch 42, best val loss: 341.1929\n",
      "⏹️ Early stopping at epoch 39, best val loss: 372.8865\n",
      "⏹️ Early stopping at epoch 39, best val loss: 346.3698\n",
      "⏹️ Early stopping at epoch 41, best val loss: 368.7378\n",
      "⏹️ Early stopping at epoch 39, best val loss: 348.7775\n",
      "⏹️ Early stopping at epoch 43, best val loss: 328.1824\n",
      "⏹️ Early stopping at epoch 40, best val loss: 353.2334\n",
      "⏹️ Early stopping at epoch 41, best val loss: 351.0441\n",
      "⏹️ Early stopping at epoch 44, best val loss: 347.9125\n",
      "⏹️ Early stopping at epoch 41, best val loss: 334.8105\n",
      "⏹️ Early stopping at epoch 41, best val loss: 334.5700\n",
      "Saved to PF_full_eta5.csv\n"
     ]
    }
   ],
   "source": [
    "# ============ EXPERIMENT PARAMETERS ============\n",
    "eps_list = [0.01, 0.05, 0.1, 0.5, 1]\n",
    "max_K = 3\n",
    "tau = 0.1\n",
    "eta_list = [5]\n",
    "hidden_node = 64\n",
    "hidden_layer = 2\n",
    "block_size =  2 \n",
    "bins = 8\n",
    "total_epoch = 500\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "rho_list_DRMC = [0.05,0.1,0.25]\n",
    "neighbor_quantile_list_CMC = [0.05,0.1,0.25]\n",
    "gamma_quantile_list_DRCMC = [0.05,0.1,0.25]\n",
    "rho_quantile_list_DRCMC = [0.05,0.1,0.25]\n",
    "quantile_level_list_OTCMC = [0.05, 0.1,0.15]\n",
    "rho_div_rho_min_list_OTCMC = [1.05,1.1,1.15]\n",
    "\n",
    "file_path = \"data/Portfolio_data.csv\"\n",
    "\n",
    "base_seed = 0\n",
    "rng_main = np.random.default_rng(base_seed)\n",
    "\n",
    "\n",
    "for eta in eta_list:\n",
    "    # === Load and parse dataset ===\n",
    "    data = pd.read_csv(file_path)\n",
    "    dim_s, dim_xi, N = 5, 399, data.shape[0]\n",
    "\n",
    "    data[\"time\"] = pd.to_datetime(data[\"time\"])\n",
    "    data.iloc[:, 1:] = data.iloc[:, 1:].astype(np.float64)\n",
    "\n",
    "    # === Preprocess for bandwidth estimation (2017–2020만 사용) ===\n",
    "    mask_scale = (data[\"time\"] >= \"2017-01-01\") & (data[\"time\"] <= \"2020-12-31\")\n",
    "    data_pre = data[mask_scale]\n",
    "    s = data_pre.iloc[:, 1:6]\n",
    "    xi = data_pre.iloc[:, 6:]\n",
    "    _, h_list = preprocess_side_info(s, xi)\n",
    "\n",
    "    # === Scale side info and returns ===\n",
    "    scaled_s = s.values / np.array(h_list)\n",
    "    data_scaled = data.copy()\n",
    "    data_scaled.loc[data_pre.index, data_scaled.columns[1:6]] = scaled_s.astype(float)\n",
    "    data_scaled.iloc[:, 6:] = data_scaled.iloc[:, 6:] / 100\n",
    "\n",
    "    # === Define CV splits ===\n",
    "    mask_cv_train = (data_scaled[\"time\"] >= \"2017-01-01\") & (data_scaled[\"time\"] <= \"2020-12-31\")\n",
    "    mask_cv_test  = (data_scaled[\"time\"] >= \"2019-01-01\") & (data_scaled[\"time\"] <= \"2020-12-31\")\n",
    "\n",
    "    data_cv_train = data_scaled[mask_cv_train]\n",
    "    data_cv_test  = data_scaled[mask_cv_test]\n",
    "\n",
    "\n",
    "    val_indices = rng_main.choice(len(data_cv_test), size=50, replace=False)\n",
    "    val_asset_indices = [rng_main.choice(dim_xi, size=399, replace=False) for _ in val_indices]\n",
    "\n",
    "    best_eps_DRMC = cv_DR_mean_CVaR_kernel(tau, eta, data_cv_train, data_cv_test, val_indices, val_asset_indices, rho_list_DRMC)\n",
    "    print(f\"[CV-DRMC Finished] best_eps_DRMC = {best_eps_DRMC}\")\n",
    "    best_quantile_CMC = cv_cond_mean_CVaR_kernel(tau, eta, data_cv_train, data_cv_test, val_indices, val_asset_indices, neighbor_quantile_list_CMC)\n",
    "    print(f\"[CV_CMC] best_quantile_CMC = {best_quantile_CMC}\")\n",
    "    best_gamma_q_DRCMC, best_eps_DRCMC = cv_DR_Winf_conditional_mean_CVaR_kernel(\n",
    "        tau, eta, data_cv_train, data_cv_test, val_indices, val_asset_indices,\n",
    "        gamma_quantile_list_DRCMC, rho_quantile_list_DRCMC\n",
    "    )\n",
    "    print(f\"[CV-DRCMC] best_gamma_q_DRCMC = {best_gamma_q_DRCMC}, best_eps_DRCMC = {best_eps_DRCMC}\")\n",
    "    best_quantile_OTCMC, best_eps_OTCMC = cv_DR_W2_conditional_mean_kernel(\n",
    "        tau, eta, data_cv_train, data_cv_test, val_indices, val_asset_indices,\n",
    "        quantile_level_list_OTCMC, rho_div_rho_min_list_OTCMC\n",
    "    )\n",
    "    print(f\"[CV-OTCMC] best_quantile_OTCMC = {best_quantile_OTCMC}, best_eps_OTCMC = {best_eps_OTCMC}\")\n",
    "    best_eps_GMM, K_cv_mean = cv_GMM(\n",
    "        tau, eta, data_cv_train, data_cv_test, val_indices, val_asset_indices,\n",
    "        eps_list, max_K, hidden_node, hidden_layer, block_size,\n",
    "        bins, total_epoch, device,\n",
    "        n_jobs=-1          \n",
    "    )\n",
    "    print(f\"[GMM-CV Finished] K={K_cv_mean}, eps={best_eps_GMM}\")\n",
    "\n",
    "    # === Define OOS train and test sets ===\n",
    "    mask_oos_train = (data_scaled[\"time\"] >= \"2019-01-01\") & (data_scaled[\"time\"] <= \"2022-12-31\")\n",
    "    data_oos_train_set = data_scaled[mask_oos_train]\n",
    "\n",
    "    # === Define OOS test set directly ===\n",
    "    mask_oos_test = (data_scaled[\"time\"] >= \"2021-01-01\") & (data_scaled[\"time\"] <= \"2021-12-31\")\n",
    "    data_oos_test = data_scaled[mask_oos_test].reset_index(drop=True)\n",
    "    val_indices = list(range(len(data_oos_test)))\n",
    "\n",
    "    # === Run trials in parallel ===\n",
    "    results = Parallel(n_jobs=-1)(   \n",
    "        delayed(run_trial)(\n",
    "            j, tau, eta,\n",
    "            data_oos_train_set, data_oos_test,\n",
    "            best_eps_DRMC, best_quantile_CMC, best_gamma_q_DRCMC, best_eps_DRCMC,\n",
    "            best_quantile_OTCMC, best_eps_OTCMC, best_eps_GMM, max_K,\n",
    "            hidden_node, hidden_layer, block_size, bins, total_epoch,\n",
    "            device\n",
    "        )\n",
    "        for j in tqdm(val_indices)\n",
    "    )\n",
    "\n",
    "    # --- Clean up / Save ---\n",
    "    results_cleaned = [r for r in results if isinstance(r, dict)]\n",
    "\n",
    "    if len(results_cleaned) == 0:\n",
    "        print(\"⚠️ No valid results; skipping save.\")\n",
    "        continue\n",
    "\n",
    "    df = pd.DataFrame(results_cleaned)\n",
    "\n",
    "    if 'Trial' not in df.columns:\n",
    "        df.insert(0, 'Trial', list(range(len(df))))  \n",
    "\n",
    "    cols = ['Trial'] + [c for c in df.columns if c != 'Trial']\n",
    "    df = df[cols]\n",
    "\n",
    "    mean_row = {'Trial': 'AVG'}\n",
    "    for col in df.columns:\n",
    "        if col != 'Trial':\n",
    "            mean_row[col] = pd.to_numeric(df[col], errors='coerce').mean(skipna=True)\n",
    "    df = pd.concat([df, pd.DataFrame([mean_row])], ignore_index=True)\n",
    "\n",
    "    save_path = f\"PF_full_eta{eta}.csv\"\n",
    "    df.to_csv(save_path, index=False)\n",
    "    print(f\"Saved to {save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
